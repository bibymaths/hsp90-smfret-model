#! /usr/bin/env python3
# -*- coding: utf-8 -*-

# SPDX-License-Identifier: BSD-3-Clause
# SPDX-FileCopyrightText: 2025 Abhinav Mishra

"""
Figures and visual outputs generated by this script are licensed under:
SPDX-License-Identifier: CC-BY-4.0
"""

# =============================================================================
#  BSD 3-Clause License
#  --------------------
#  Copyright (c) 2025, Abhinav Mishra
#  All rights reserved.
#
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions are met:
#
#  1. Redistributions of source code must retain the above copyright notice,
#     this list of conditions and the following disclaimer.
#
#  2. Redistributions in binary form must reproduce the above copyright notice,
#     this list of conditions and the following disclaimer in the documentation
#     and/or other materials provided with the distribution.
#
#  3. Neither the name of the copyright holder nor the names of its contributors
#     may be used to endorse or promote products derived from this software
#     without specific prior written permission.
#
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS”
#  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
#  OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
#  WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
#  OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
#  ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
# =============================================================================
# =============================================================================
#  Hsp90 Single-Molecule FRET Analysis Pipeline
#  --------------------------------------------
#
#  This module implements a complete end-to-end analysis pipeline for
#  single-molecule FRET (smFRET) time-series collected from tracking
#  experiments (HDF5 *.tracks) in the Hügel lab format.
#
#  The pipeline performs:
#
#    1. **HDF5 Inspection and Trajectory Extraction**
#       - Reads raw per-molecule donor–acceptor trajectories.
#       - Filters by manual flags, donor-excitation frames, and physical bounds.
#       - Discards short or noisy traces.
#       - Exports each trajectory to compact CSV files.
#
#    2. **Uniform Time-Grid Interpolation**
#       - Interpolates irregular raw sampling onto a global time grid.
#       - Builds a combined trajectory matrix: shape (T × N).
#       - Optionally trims all data to the first 100 seconds.
#
#    3. **Three-State Hsp90 Kinetic Model**
#       - Implements the O ↔ I ↔ C conformational cycle with
#         state-dependent bleaching (O→B, I→B, C→B).
#       - ODE solved via `solve_ivp` with a numba-accelerated RHS.
#       - Enforces physically meaningful probabilities (0 ≤ P ≤ 1).
#
#    4. **Static Subpopulation + Dynamic Mixture**
#       - Total FRET = f_dyn * E_dyn(t) + (1 − f_dyn) * E_static.
#       - Fully parameterized model with:
#           · 7 kinetic rates,
#           · 3 ordered FRET levels (E_open < E_inter < E_closed),
#           · 2 initial conditions,
#           · static fraction and level.
#
#    5. **Global Ensemble Fitting**
#       - Fits ensemble-averaged trajectories using weighted nonlinear regression.
#       - Heteroscedastic weighting from time-wise trajectory variance.
#       - Optional multistart optimization to avoid local minima.
#
#    6. **Condition-Resolved Fitting**
#       - Automatically parses trajectory metadata
#         (construct, experiment ID, particle index).
#       - Fits the model per condition, per construct, or globally.
#
#    7. **Diagnostics & Visualization**
#       - Overlay of hundreds of raw trajectories.
#       - Ensemble mean ± 1 SD vs. model fit.
#       - Residual plots.
#       - Quality-control summary for each condition.
#
#    8. **Bootstrap Uncertainty Quantification**
#       - Resamples trajectories with replacement.
#       - Produces parameter distributions + 95% confidence intervals.
#
#    9. **Global Sensitivity Analysis**
#       - Sobol sensitivity indices (first-order, total-order, second-order).
#       - Identifies dominant kinetic and FRET-level parameters.
#
#  The code is structured to be:
#     • numerically stable,
#     • physically constrained,
#     • parallelizable (joblib),
#     • reproducible,
#     • and compatible with future extension (MLE, Bayesian inference, GMM, etc.).
#
#  This module serves as a complete and transparent computational counterpart to
#  the experimental ground-truth measurements described in the accompanying paper.
# =============================================================================

import argparse
import logging
import os
import shutil
import warnings
from dataclasses import dataclass
from pathlib import Path
from typing import Tuple, List, Optional, cast

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from SALib.analyze import sobol
from SALib.sample import saltelli
from joblib import Parallel, delayed
from numba import njit
from numpy.typing import NDArray
from rich.console import Console
from rich.logging import RichHandler
from rich.theme import Theme
from scipy.integrate import solve_ivp
from scipy.integrate._ivp.ivp import OdeResult
from scipy.optimize import curve_fit

warnings.filterwarnings("ignore", category=DeprecationWarning, module="SALib")

# -----------------------
# Command-line arguments
# -----------------------
parser = argparse.ArgumentParser(
    description="Conformational Hsp90 FRET global fitting, sensitivity, and bootstrap."
)

parser.add_argument("--n-states", type=int, choices=[2, 3], default=2,
                    help="Number of conformational states (2 or 3).")

parser.add_argument(
    "--outdir",
    type=str,
    default="results",
    help="Output directory for tables/plots (default: %(default)s)",
)
parser.add_argument(
    "--multistarts",
    type=int,
    default=5,
    help="Number of multi-starts per fit (default: %(default)s)",
)
parser.add_argument(
    "--bootstraps",
    type=int,
    default=10,
    help="Number of bootstrap replicates per group (default: %(default)s)",
)
parser.add_argument(
    "--cores",
    type=int,
    default=4,
    help="Number of CPU cores for Joblib parallel sections (default: %(default)s)",
)

args = parser.parse_args()

# -----------------------
# Results directory
# -----------------------
outdir = Path(args.outdir)
outdir.mkdir(parents=True, exist_ok=True)

# -----------------------
# Rich logging setup
# -----------------------
custom_theme = Theme({
    "log.time": "dim cyan",
    "log.level.debug": "dim",
    "log.level.info": "bold white",
    "log.level.warning": "bold yellow",
    "log.level.error": "bold red",
    "log.level.critical": "bold white on red",
})

term_width = shutil.get_terminal_size(fallback=(220, 24)).columns

console = Console(
    theme=custom_theme,
    width=term_width,
    soft_wrap=False,
    color_system="truecolor",
    force_terminal=True,
)

handler = RichHandler(
    console=console,
    rich_tracebacks=True,
    markup=True,
    show_time=True,
    show_level=True,
    enable_link_path=False,
)

logging.basicConfig(
    level=logging.INFO,
    format="%(message)s",
    handlers=[handler],
    datefmt="[%X]",
)

logger = logging.getLogger(__name__)


def log_df(df: pd.DataFrame,
           title: str | None = None,
           level: int = logging.INFO,
           max_rows: int = 60) -> None:
    """
    Log a DataFrame to the rich logger with optional title and row limit.

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame to log.
    title : str | None
        Optional title to display above the DataFrame.
    level : int
        Logging level (default: logging.INFO).
    max_rows : int
        Maximum number of rows to display (default: 60).
    Returns
    -------
    None
    """

    if df is None or df.empty:
        logger.log(level, "[bold yellow]DataFrame is empty[/bold yellow]")
        return

    if len(df) > max_rows:
        df = df.head(max_rows)

    with pd.option_context(
            "display.width", console.width,
            "display.max_columns", None,
            "display.max_colwidth", console.width,
    ):
        if title:
            logger.log(level, f"[bold cyan]{title}[/bold cyan]")
        logger.log(level, "\n" + df.to_string(index=True))

@dataclass
class Hsp90Params2State:
    """
    Two-state conformational model with bleaching:
        O <-> C, each can irreversibly bleach to B with rate k_B.

    Parameters
    -------------

    k_OC     : float

    """
    # Conformational rates
    k_OC: float  # Open -> Closed (1/s)
    k_CO: float  # Closed -> Open (1/s)

    # State-dependent bleaching
    k_BO: float  # O -> B
    k_BC: float  # C -> B

    # FRET levels
    E_open: float
    E_closed: float

    # Initial probability (P_C0 = 1 - P_O0)
    P_O0: float

@dataclass
class Hsp90Params3State:
    """
    Three-state conformational model with bleaching:
      O <-> I <-> C, each can irreversibly bleach to B with rate k_B.

    We explicitly track P_O, P_I, P_C; P_B = 1 - P_O - P_I - P_C.
    FRET is from O, I, C; bleached B is assumed dark (E_bleach = 0).

    Parameters
    ----------
    k_OI : float
        Open -> Intermediate rate (1/s)
    k_IO : float
        Intermediate -> Open rate (1/s)
    k_IC : float
        Intermediate -> Closed rate (1/s)
    k_CI : float
        Closed -> Intermediate rate (1/s)
    k_BO : float
        Open -> Bleached rate (1/s)
    k_BI : float
        Intermediate -> Bleached rate (1/s)
    k_BC : float
        Closed -> Bleached rate (1/s)
    E_open : float
        FRET efficiency in Open state
    E_inter : float
        FRET efficiency in Intermediate state
    E_closed : float
        FRET efficiency in Closed state
    P_O0 : float
        Initial probability of being in Open state
    P_C0 : float
        Initial probability of being in Closed state

    Returns
    -------
    None

    """
    # Conformational rates
    k_OI: float  # Open -> Intermediate rate (1/s)
    k_IO: float  # Intermediate -> Open rate (1/s)
    k_IC: float  # Intermediate -> Closed rate (1/s)
    k_CI: float  # Closed -> Intermediate rate (1/s)

    # State-dependent bleaching
    k_BO: float  # O -> B
    k_BI: float  # I -> B
    k_BC: float  # C -> B

    # FRET levels
    E_open: float  # FRET in Open state
    E_inter: float  # FRET in Intermediate state
    E_closed: float  # FRET in Closed state

    # Initial probabilities (P_I0 = 1 - P_O0 - P_C0, P_B0 = 0)
    P_O0: float  # Initial Open probability
    P_C0: float  # Initial Closed probability

@dataclass
class Hsp90Fit2State:
    """
    Container for a full fit: kinetics + static subpopulation.

    Parameters
    ----------
    params : Hsp90Params2State
        Fitted kinetic parameters.
    f_dyn : float
        Fraction of molecules following the kinetic model.
    E_static : float
        FRET level of static subpopulation.
    Returns
    -------
    None
    """
    params: Hsp90Params2State
    f_dyn: float  # fraction of molecules following the kinetic model
    E_static: float  # FRET level of static subpopulation

@dataclass
class Hsp90Fit3State:
    """
    Container for a full fit: kinetics + static subpopulation.

    Parameters
    ----------
    params : Hsp90Params3State
        Fitted kinetic parameters.
    f_dyn : float
        Fraction of molecules following the kinetic model.
    E_static : float
        FRET level of static subpopulation.
    Returns
    -------
    None
    """
    params: Hsp90Params3State
    f_dyn: float  # fraction of molecules following the kinetic model
    E_static: float  # FRET level of static subpopulation


def fit_to_df(fit: Hsp90Fit3State) -> pd.DataFrame:
    """
    Flatten a single Hsp90Fit3State into a 1-row DataFrame.

    Parameters
    ----------
    fit : Hsp90Fit3State
        Fitted model.
    Returns
    -------
    pd.DataFrame
        One-row DataFrame with all fit parameters.
    """
    p = fit.params
    row = {
        "k_OI": p.k_OI,
        "k_IO": p.k_IO,
        "k_IC": p.k_IC,
        "k_CI": p.k_CI,
        "k_BO": p.k_BO,
        "k_BI": p.k_BI,
        "k_BC": p.k_BC,
        "E_open": p.E_open,
        "E_inter": p.E_inter,
        "E_closed": p.E_closed,
        "P_O0": p.P_O0,
        "P_C0": p.P_C0,
        "f_dyn": fit.f_dyn,
        "E_static": fit.E_static,
    }
    return pd.DataFrame([row])

def fit_to_df_2state(fit: Hsp90Fit2State) -> pd.DataFrame:
    """
    Flatten a single Hsp90Fit2State into a 1-row DataFrame.

    Parameters
    ----------
    fit : Hsp90Fit3State
        Fitted model.
    Returns
    -------
    pd.DataFrame
        One-row DataFrame with all fit parameters.
    """
    p = fit.params
    row = {
        "k_OC": p.k_OC,
        "k_CO": p.k_CO,
        "k_BO": p.k_BO,
        "k_BC": p.k_BC,
        "E_open": p.E_open,
        "E_closed": p.E_closed,
        "P_O0": p.P_O0,
        "f_dyn": fit.f_dyn,
        "E_static": fit.E_static,
    }
    return pd.DataFrame([row])

@njit("Tuple((float64, float64, float64))(float64, float64, float64)", cache=True, fastmath=True, nogil=True)
def ordered_levels(e0, d1, d2):
    """
    Map unconstrained parameters to ordered FRET levels:
    Eo = sigmoid(e0)
    Ei = Eo + sigmoid(d1) * (1 - Eo)
    Ec = Ei + sigmoid(d2) * (1 - Ei)

    Parameters
    ----------
    e0 : float
        Unconstrained parameter for Eo.
    d1 : float
        Unconstrained increment parameter for Ei.
    d2 : float
        Unconstrained increment parameter for Ec.
    Returns
    -------
    Eo : float
        FRET efficiency in Open state (0 < Eo < 1).
    Ei : float
        FRET efficiency in Intermediate state (Eo < Ei < 1).
    Ec : float
        FRET efficiency in Closed state (Ei < Ec < 1).
    """
    Eo = 1 / (1 + np.exp(-e0))  # (0,1)
    inc1 = (1 / (1 + np.exp(-d1))) * (1 - Eo)  # (0, 1-Eo)
    Ei = Eo + inc1
    inc2 = (1 / (1 + np.exp(-d2))) * (1 - Ei)  # (0, 1-Ei)
    Ec = Ei + inc2
    return Eo, Ei, Ec


@njit("float64[:](float64, float64[:], float64[:])",cache=True, fastmath=True, nogil=True)
def rhs_hsp90_2state_numba(t, y, params):
    k_OC, k_CO, k_BO, k_BC = params
    P_O, P_C = y[0], y[1]
    dP_O = -k_OC * P_O + k_CO * P_C - k_BO * P_O
    dP_C =  k_OC * P_O - k_CO * P_C - k_BC * P_C
    return np.array([dP_O, dP_C], dtype=np.float64)


def model_fret_2state(t_eval: np.ndarray, p: Hsp90Params2State) -> np.ndarray:
    P_O0 = float(np.clip(p.P_O0, 0.0, 1.0))
    P_C0 = 1.0 - P_O0
    y0 = np.array([P_O0, P_C0], dtype=float)

    k_params = np.array([p.k_OC, p.k_CO, p.k_BO, p.k_BC], dtype=float)

    sol = solve_ivp(
        fun=rhs_hsp90_2state_numba,
        t_span=(float(np.min(t_eval)), float(np.max(t_eval))),
        y0=y0,
        t_eval=t_eval,
        vectorized=False,
        args=(k_params,),
    )
    if not sol.success:
        return np.full_like(t_eval, np.nan, dtype=float)

    P = np.clip(sol.y, 0.0, 1.0)

    # If numeric drift makes P_O+P_C > 1, renormalize
    S = P.sum(axis=0)
    bad = S > 1.0
    if np.any(bad):
        P[:, bad] /= S[bad]

    Eo = float(np.clip(p.E_open, 0.0, 1.0))
    Ec = float(np.clip(p.E_closed, 0.0, 1.0))
    return Eo * P[0] + Ec * P[1]

@njit("float64[:](float64, float64[:], float64[:])", cache=True, fastmath=True, nogil=True)
def rhs_hsp90_numba(t: float, y: np.ndarray, params: np.ndarray) -> np.ndarray:
    """
    Numba-optimized ODE for P_O, P_I, P_C with bleaching.

    Parameters
    ----------
    t : float
        Current time (not used in this ODE as it's time-invariant).
    y : ndarray
        Current state vector [P_O, P_I, P_C].
    params : ndarray
        Kinetic parameters: [k_OI, k_IO, k_IC, k_CI, k_BO, k_BI, k_BC].
    Returns
    -------
    dPdt : ndarray
        Time derivatives [dP_O/dt, dP_I/dt, dP_C/dt].
    """
    k_OI, k_IO, k_IC, k_CI, k_BO, k_BI, k_BC = params  # 7 kinetic params now
    P_O, P_I, P_C = y[0], y[1], y[2]
    dP_O = -k_OI * P_O + k_IO * P_I - k_BO * P_O
    dP_I = k_OI * P_O - (k_IO + k_IC + k_BI) * P_I + k_CI * P_C
    dP_C = k_IC * P_I - k_CI * P_C - k_BC * P_C
    return np.array([dP_O, dP_I, dP_C], dtype=np.float64)


@njit(cache=True, fastmath=True, nogil=True)
def jac_hsp90_numba(t, y, params):
    """
    Analytical Jacobian for the 3-state system.
    Returns df/dy (the matrix of partial derivatives).
    """
    k_OI, k_IO, k_IC, k_CI, k_BO, k_BI, k_BC = params

    # Rows correspond to d(dP_O)/dy, d(dP_I)/dy, d(dP_C)/dy
    # Columns correspond to P_O, P_I, P_C
    jac = np.zeros((3, 3), dtype=np.float64)

    # d(dP_O)/dt = -k_OI*P_O + k_IO*P_I - k_BO*P_O
    jac[0, 0] = -(k_OI + k_BO)
    jac[0, 1] = k_IO
    jac[0, 2] = 0.0

    # d(dP_I)/dt = k_OI*P_O - (k_IO + k_IC + k_BI)*P_I + k_CI*P_C
    jac[1, 0] = k_OI
    jac[1, 1] = -(k_IO + k_IC + k_BI)
    jac[1, 2] = k_CI

    # d(dP_C)/dt = k_IC*P_I - (k_CI + k_BC)*P_C
    jac[2, 0] = 0.0
    jac[2, 1] = k_IC
    jac[2, 2] = -(k_CI + k_BC)

    return jac

def model_fret_3state(t_eval: np.ndarray, p: Hsp90Params3State) -> np.ndarray:
    """
    Dynamic part only: E_dyn(t) = E_O*P_O + E_I*P_I + E_C*P_C.

    This version enforces:
      - 0 <= P_O0, P_I0, P_C0 <= 1
      - P_O0 + P_I0 + P_C0 = 1   (all non-bleached states)
    and clamps small numerical negatives in the solution.
    """
    # --- initial probabilities with normalization / clipping ---
    P_O0 = np.asarray(float(p.P_O0)).item()
    P_C0 = np.asarray(float(p.P_C0)).item()

    # clip raw guesses to [0,1] first (hard box)
    P_O0 = np.clip(P_O0, 0.0, 1.0)
    P_C0 = np.clip(P_C0, 0.0, 1.0)

    # provisional P_I0 from "whatever is left"
    P_I0 = 1.0 - P_O0 - P_C0

    # if that went negative (P_O0 + P_C0 > 1), renormalize
    if P_I0 < 0.0:
        total = max(P_O0 + P_C0, 1e-12)
        P_O0 /= total
        P_C0 /= total
        P_I0 = 0.0
    else:
        # all three non-negative; now renormalize to sum exactly 1
        total = P_O0 + P_I0 + P_C0
        if total <= 0.0:
            # completely pathological guess -> bail with NaNs
            return np.full_like(t_eval, np.nan, dtype=float)
        P_O0 /= total
        P_I0 /= total
        P_C0 /= total

    y0 = np.array([P_O0, P_I0, P_C0], dtype=float)

    # kinetic parameters (already box-constrained in the fit)
    k_params = np.array(
        [p.k_OI, p.k_IO, p.k_IC, p.k_CI, p.k_BO, p.k_BI, p.k_BC],
        dtype=float
    )

    sol = cast(OdeResult, cast(object, solve_ivp(
        fun=rhs_hsp90_numba,
        t_span=(t_eval.min(), t_eval.max()),
        y0=y0,
        t_eval=t_eval,
        vectorized=False,
        args=(k_params,),
        # jac=jac_hsp90_numba,
        # method='LSODA',
    )
    ))

    if not sol.success:
        return np.full_like(t_eval, np.nan, dtype=float)

    # clamp tiny negative / >1 probabilities from numerical error
    P = np.clip(sol.y, 0.0, 1.0)

    # optional: renormalize so P_O + P_I + P_C <= 1 (rest is bleached)
    S = P.sum(axis=0)
    mask = S > 1.0
    if np.any(mask):
        P[:, mask] /= S[mask]

    P_O_t = P[0]
    P_I_t = P[1]
    P_C_t = P[2]

    # also make sure FRET levels live in [0,1]
    E_open = np.clip(p.E_open, 0.0, 1.0)
    E_inter = np.clip(p.E_inter, 0.0, 1.0)
    E_closed = np.clip(p.E_closed, 0.0, 1.0)

    E_t = (
            E_open * P_O_t +
            E_inter * P_I_t +
            E_closed * P_C_t
    )
    return E_t


def model_total_fret(t_eval: np.ndarray, fit: Hsp90Fit3State) -> np.ndarray:
    """
    Total FRET: E_total(t) = f_dyn * E_dyn(t) + (1 - f_dyn) * E_static

    Parameters
    ----------
    t_eval : ndarray
        Time points to evaluate.
    fit : Hsp90Fit3State
        Fitted model.
    Returns
    -------
    ndarray
        Total FRET efficiency at each time point.
    """
    p = fit.params
    if isinstance(p, Hsp90Params3State):
        # Hsp90Params3State
        E_dyn = model_fret_3state(t_eval, p)
    else:
        # Hsp90Params2State
        E_dyn = model_fret_2state(t_eval, p)
    return fit.f_dyn * E_dyn + (1.0 - fit.f_dyn) * fit.E_static


# ----------------------------------------------------------------------
# Data loading
# ----------------------------------------------------------------------

def load_combined_matrix(path: Path) -> Tuple[np.ndarray, np.ndarray, List[str]]:
    """
    Load combined_fret_matrix CSV file into time grid and FRET matrix.

    Parameters
    ----------
    path : Path
        Path to combined_fret_matrix CSV file.
    Returns
    -------
    t : (T,) array
        Time grid.
    E_mat : (T, N) array
        FRET trajectories matrix.
    traj_cols : list of str
        Names of trajectory columns (same order as E_mat columns).
    """
    df = pd.read_csv(path)
    if "time_s" not in df.columns:
        raise ValueError("Expected a 'time_s' column in the combined matrix.")

    t = df["time_s"].values
    traj_cols = [c for c in df.columns if c != "time_s"]
    E_mat = df[traj_cols].to_numpy()

    row_valid = np.isfinite(E_mat).any(axis=1)
    t = t[row_valid]
    E_mat = E_mat[row_valid, :]

    return t, E_mat, traj_cols


def parse_column_metadata(col_names: List[str]) -> pd.DataFrame:
    """
    Parse combined_fret_matrix trajectory column names into a metadata DataFrame.

    Expected format (from your export code):
        <construct>_<exp_id>_p<particle>
    e.g. "Hsp90_409_601_241107_p00001"

    Returns
    -------
    meta : DataFrame with columns:
        - col: original column name
        - construct: e.g. "Hsp90_409_601" or "esDNA"
        - exp_id: e.g. "241107"
        - particle: string/ID after 'p'
        - condition: default grouping key "<construct>_<exp_id>"
    """
    records = []
    for c in col_names:
        if c == "time_s":
            continue

        # Split from the right: [..., construct, exp_id, pXXXXX]
        parts = c.split("_")
        if len(parts) < 3:
            # Fallback: treat whole name as "construct"
            construct = c
            exp_id = "unknown"
            particle = "unknown"
        else:
            particle = parts[-1]  # e.g. "p00001"
            exp_id = parts[-2]  # e.g. "241107"
            construct = "_".join(parts[:-2])  # e.g. "Hsp90_409_601" or "esDNA"

        condition = f"{construct}_{exp_id}"
        # condition = f"{construct}"
        records.append((c, construct, exp_id, particle, condition))

    meta = pd.DataFrame(
        records,
        columns=["col", "construct", "exp_id", "particle", "condition"]
    )
    return meta


def subset_matrix_by_columns(
        t: np.ndarray,
        E_mat: np.ndarray,
        all_cols: List[str],
        cols_subset: List[str]
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Given the full time grid and matrix, extract a sub-matrix restricted to
    a subset of trajectory columns, and remove rows where all subset entries are NaN.

    Parameters
    ----------
    t : (T,) array
        Full time grid.
    E_mat : (T, N) array
        FRET trajectories for all columns in all_cols.
    all_cols : list of str
        Names of all trajectory columns (corresponding to E_mat columns).
    cols_subset : list of str
        Names of columns to keep for this subset.

    Returns
    -------
    t_sub : (T_sub,) array
        Time grid for which at least one trajectory in the subset is finite.
    E_sub : (T_sub, N_sub) array
        Subset FRET matrix.
    """
    name_to_idx = {c: i for i, c in enumerate(all_cols)}
    idx = [name_to_idx[c] for c in cols_subset if c in name_to_idx]

    if not idx:
        raise ValueError("No matching columns found for subset.")

    E_sub_full = E_mat[:, idx]
    row_valid = np.isfinite(E_sub_full).any(axis=1)
    t_sub = t[row_valid]
    E_sub = E_sub_full[row_valid, :]
    return t_sub, E_sub


def compute_ensemble_metrics(
        t: np.ndarray,
        E_mat: np.ndarray,
        fit: Hsp90Fit3State
) -> dict:
    """
    Compute ensemble RMSE and R^2 for a given condition and fitted model.

    Uses ensemble mean vs model prediction.

    Parameters
    ----------
    t : (T,) array
        Time grid.
    E_mat : (T, N) array
        FRET trajectories matrix.
    fit : Hsp90Fit3State
        Fitted model.
    Returns
    -------
    dict
        Dictionary with keys:
            - rmse : float
                Root Mean Square Error between mean observed and model.
            - r2 : float
                Coefficient of determination.
            - n_time : int
                Number of time points used in the comparison.
            - n_traj : int
                Number of trajectories in E_mat.
    """
    row_valid = np.isfinite(E_mat).any(axis=1)
    t_plot = t[row_valid]
    E_plot = E_mat[row_valid, :]

    if t_plot.size == 0:
        return dict(rmse=np.nan, r2=np.nan, n_time=0, n_traj=0)

    E_mean = np.nanmean(E_plot, axis=1)
    E_model = model_total_fret(t_plot, fit)

    mask = np.isfinite(E_mean) & np.isfinite(E_model)
    E_obs = E_mean[mask]
    E_mod = E_model[mask]

    if E_obs.size == 0:
        return dict(rmse=np.nan, r2=np.nan, n_time=0, n_traj=E_plot.shape[1])

    residuals = E_obs - E_mod
    rmse = np.sqrt(np.mean(residuals ** 2))
    ss_res = np.sum(residuals ** 2)
    ss_tot = np.sum((E_obs - E_obs.mean()) ** 2)
    r2 = 1.0 - ss_res / ss_tot if ss_tot > 0 else np.nan

    return dict(
        rmse=float(rmse),
        r2=float(r2),
        n_time=int(len(E_obs)),
        n_traj=int(E_mat.shape[1]),
    )


def _fit_single_condition_worker(
        key: str,
        t: np.ndarray,
        E_mat: np.ndarray,
        col_names: List[str],
        meta: pd.DataFrame,
        group_by: str,
        n_starts: int,
        n_jobs: int
) -> Optional[Tuple[str, Hsp90Fit3State, dict]]:
    """
    Internal worker for parallel fitting of a single condition.

    Parameters
    ----------
    key : str
        Condition key to fit.
    t : (T,) array
        Global time grid.
    E_mat : (T, N) array
        Full combined FRET matrix.
    col_names : list of str
        Names of trajectory columns (same order as E_mat columns).
    meta : DataFrame
        Metadata DataFrame parsed from column names.
    group_by : str
        Grouping key used ("condition", "construct", or "exp_id").
    n_starts : int
        Number of multi-starts for fitting.
    Returns
    -------
    Optional[Tuple[str, Hsp90Fit3State, dict]]
        Tuple of (key, fit, metrics) if successful; None if failed.
    """
    cols_subset = meta.loc[meta[group_by] == key, "col"].tolist()
    if not cols_subset:
        return None

    # 1. Subset data for this specific condition
    t_sub, E_sub = subset_matrix_by_columns(t, E_mat, col_names, cols_subset)

    # 2. Perform the heavy computational fit
    # n_job = 1 for outermost parallelism
    # otherwise, risk for overhead subscription --> russian doll effect
    if args.n_states == 3:
        fit = fit_global_3state(t_sub, E_sub, n_starts=n_starts, n_jobs=1)
    else:
        fit = fit_global_2state(t_sub, E_sub, n_starts=n_starts, n_jobs=1)

    # 3. Compute metrics
    metrics = compute_ensemble_metrics(t_sub, E_sub, fit)

    # 4. Pack results into a dictionary record
    # 4. Pack results into a dictionary record
    p = fit.params

    base = dict(
        group_by=group_by,
        group_key=key,
        n_traj=metrics["n_traj"],
        n_time=metrics["n_time"],
        rmse=metrics["rmse"],
        r2=metrics["r2"],
        f_dyn=fit.f_dyn,
        E_static=fit.E_static,
    )

    if args.n_states == 3:
        rec = dict(
            **base,
            k_OI=p.k_OI, k_IO=p.k_IO, k_IC=p.k_IC, k_CI=p.k_CI,
            k_BO=p.k_BO, k_BI=p.k_BI, k_BC=p.k_BC,
            E_open=p.E_open, E_inter=p.E_inter, E_closed=p.E_closed,
            P_O0=p.P_O0, P_C0=p.P_C0,
        )
    else:
        rec = dict(
            **base,
            k_OC=p.k_OC, k_CO=p.k_CO,
            k_BO=p.k_BO, k_BC=p.k_BC,
            E_open=p.E_open, E_closed=p.E_closed,
            P_O0=p.P_O0,
        )

    return (key, fit, rec)


def fit_all_conditions(
        t: np.ndarray,
        E_mat: np.ndarray,
        col_names: List[str],
        group_by: str = "condition",
        do_plots: bool = False,
        max_overlay_traces: int = 100,
        n_starts: int = 5,
        n_jobs: int = 4,
) -> Tuple[pd.DataFrame, dict]:
    """
    Fit the 3-state+bleaching+static model separately for each condition.

    Parameters
    ----------
    t : (T,) array
        Global time grid.
    E_mat : (T, N) array
        Full combined FRET matrix.
    col_names : list of str
        Names of trajectory columns (same order as E_mat columns).
    group_by : {"condition", "construct", "exp_id"}
        How to group trajectories:
        - "condition": construct+exp_id (default, per coverslip/day)
        - "construct": pool all days for each construct
        - "exp_id": per date across constructs (usually less useful)
    do_plots : bool
        If True, make per-condition time plots using plot_hsp90_fit_time.
    max_overlay_traces : int
        Maximum number of individual trajectories to overlay per condition.

    Returns
    -------
    summary_df : DataFrame
        One row per condition with fitted parameters and metrics.
    fits : dict
        Mapping: condition_key -> Hsp90Fit3State
    """
    meta = parse_column_metadata(col_names)
    if group_by not in meta.columns:
        raise ValueError(f"group_by must be one of {', '.join(['condition', 'construct', 'exp_id'])}")

    group_keys = sorted(meta[group_by].unique())
    logger.info(f"Starting parallel fit for {len(group_keys)} groups...")

    # --- PARALLEL FITTING ---
    # n_jobs=-1 uses all available CPU cores.
    # verbose=1 provides a progress update in the logger.info.
    results = Parallel(n_jobs=n_jobs, verbose=0)(
        delayed(_fit_single_condition_worker)(
            key, t, E_mat, col_names, meta, group_by, n_starts, n_jobs
        ) for key in group_keys
    )
    # ------------------------

    # Unpack results from parallel workers
    fits_list = []
    fit_dict = {}
    for res in results:
        if res is not None:
            key, fit, rec = res
            fit_dict[key] = fit
            fits_list.append(rec)

    # Create summary DataFrame
    if not fits_list:
        if args.n_states == 3:
            summary_df = pd.DataFrame(columns=[
                "group_by", "group_key", "n_traj", "n_time", "rmse", "r2",
                "k_OI", "k_IO", "k_IC", "k_CI", "k_BO", "k_BI", "k_BC",
                "E_open", "E_inter", "E_closed", "P_O0", "P_C0", "f_dyn", "E_static"
            ])
        else:
            summary_df = pd.DataFrame(columns=[
                "group_by", "group_key", "n_traj", "n_time", "rmse", "r2",
                "k_OC", "k_CO", "k_BO", "k_BC", "E_open", "E_closed", "P_O0", "f_dyn", "E_static"
            ])
    else:
        summary_df = pd.DataFrame(fits_list)

    # --- SEQUENTIAL PLOTTING --
    # Must be done serially because Matplotlib is not thread-safe.
    if do_plots and fit_dict:
        for key in sorted(fit_dict.keys()):
            logger.info(f"  Plotting {key}...")
            # Re-subset data just for plotting
            cols = meta.loc[meta[group_by] == key, "col"].tolist()
            t_sub, E_sub = subset_matrix_by_columns(t, E_mat, col_names, cols)

            plot_hsp90_fit_time(
                t_sub, E_sub, fit_dict[key],
                n_traces_overlay=max_overlay_traces,
                random_seed=0,
                condition_key=key
            )

    return summary_df, fit_dict


# ----------------------------------------------------------------------
# Global fitting using curve_fit (ensemble mean)
# ----------------------------------------------------------------------
def fret_wrapper_2s(
        t_in,
        k_oc, k_co, k_bo, k_bc,
        e_o, e_c,
        p_o0,
        f_dyn, e_static
):
    params = Hsp90Params2State(
        k_OC=k_oc, k_CO=k_co,
        k_BO=k_bo, k_BC=k_bc,
        E_open=e_o, E_closed=e_c,
        P_O0=p_o0
    )
    fit = Hsp90Fit3State(params=params, f_dyn=f_dyn, E_static=e_static)
    return model_total_fret(np.asarray(t_in, dtype=float), fit)

def fit_global_2state(
        t: np.ndarray,
        E_mat: np.ndarray,
        theta0: np.ndarray = None,
        n_starts: int = 5,
        n_jobs: int = 1
) -> Hsp90Fit3State:
    """
    Fit 2-state O<->C + bleaching (O->B, C->B) + static mixture.

    Parameters (9 total):
      [k_OC, k_CO, k_BO, k_BC, E_open, E_closed, P_O0, f_dyn, E_static]
    """

    # --- prepare ensemble mean and sigma (same as your 3-state) ---
    row_valid = np.isfinite(E_mat).any(axis=1)
    t_fit = t[row_valid]
    E_mean = np.nanmean(E_mat[row_valid, :], axis=1)

    mask = np.isfinite(E_mean)
    t_fit = t_fit[mask]
    E_fit = E_mean[mask]

    E_std_all = np.nanstd(E_mat[row_valid, :], axis=1)
    sigma = E_std_all[mask]
    sigma = np.where(np.isfinite(sigma) & (sigma > 1e-6), sigma, 1e-6)

    if t_fit.size == 0:
        raise RuntimeError("No valid data for 2-state fitting.")

    # --- defaults ---
    if theta0 is None:
        # conservative guesses; keep units in 1/s
        theta0 = np.array([
            0.02, 0.02,      # k_OC, k_CO
            0.005, 0.010,    # k_BO, k_BC
            0.40, 0.75,      # E_open, E_closed
            0.50,            # P_O0
            0.70, 0.18       # f_dyn, E_static
        ], dtype=float)

    lower = np.array([
        0.0, 0.0,      # k_OC, k_CO
        0.0, 0.0,      # k_BO, k_BC
        0.0, 0.0,      # E_open, E_closed
        0.0,           # P_O0
        0.0, 0.0       # f_dyn, E_static
    ], dtype=float)

    upper = np.array([
        10.0, 10.0,    # k_OC, k_CO
        2.0, 2.0,      # k_BO, k_BC
        1.0, 1.0,      # E_open, E_closed
        1.0,           # P_O0
        1.0, 1.0       # f_dyn, E_static
    ], dtype=float)

    # --- multistart (same logic as your 3-state) ---
    rng = np.random.default_rng(0)
    start_configs: list[tuple[int, np.ndarray, str]] = []
    for s in range(n_starts):
        if s == 0:
            theta_start = theta0.copy()
            kind = "base"
        else:
            jitter = 1.0 + 0.3 * rng.normal(size=theta0.size)
            theta_start = theta0 * jitter
            theta_start = np.clip(theta_start, lower + 1e-8, upper - 1e-8)
            kind = "jitter"
        start_configs.append((s, theta_start, kind))

    def _single_start_worker(cfg: tuple[int, np.ndarray, str]) -> dict:
        s, theta_start, kind = cfg

        res_obj = curve_fit(
            fret_wrapper_2s,
            t_fit, E_fit,
            p0=theta_start,
            bounds=(lower, upper),
            sigma=sigma,
            absolute_sigma=True,
            maxfev=20000,
        )
        popt, _pcov = res_obj

        E_model = fret_wrapper_2s(t_fit, *popt)
        mask_obj = np.isfinite(E_fit) & np.isfinite(E_model)
        if not np.any(mask_obj):
            return {"s": s, "kind": kind, "ok": False, "msg": "no valid points", "popt": None,
                    "cost": np.inf, "rmse": np.inf}

        resid = E_fit[mask_obj] - E_model[mask_obj]
        cost = float(np.mean(resid ** 2))
        rmse = float(np.sqrt(cost))
        return {"s": s, "kind": kind, "ok": True, "msg": "", "popt": popt, "cost": cost, "rmse": rmse}

    results = Parallel(n_jobs=n_jobs, verbose=0)(
        delayed(_single_start_worker)(cfg) for cfg in start_configs
    )

    best_popt = None
    best_cost = np.inf
    for res in results:
        s = res["s"]
        kind = res["kind"]
        if not res["ok"]:
            logger.info(f"[fit_2state] multi-start {s + 1}/{n_starts} ({kind}) failed: {res['msg']}")
            continue

        logger.info(f"[fit_2state] multi-start {s + 1}/{n_starts} ({kind}): RMSE = {res['rmse']:.6f}")
        if res["cost"] < best_cost:
            best_cost = res["cost"]
            best_popt = res["popt"]

    if best_popt is None:
        raise RuntimeError("fit_global_2state: all multi-start attempts failed.")

    logger.info(f"[fit_2state] selected solution with RMSE = {np.sqrt(best_cost):.6f}")

    (k_oc, k_co, k_bo, k_bc,
     e_o, e_c,
     p_o0,
     f_dyn, e_static) = best_popt

    params = Hsp90Params2State(
        k_OC=float(k_oc),
        k_CO=float(k_co),
        k_BO=float(k_bo),
        k_BC=float(k_bc),
        E_open=float(e_o),
        E_closed=float(e_c),
        P_O0=float(p_o0),
    )
    return Hsp90Fit3State(params=params, f_dyn=float(f_dyn), E_static=float(e_static))

def fret_wrapper_3s(
        t_in,
        k_oi, k_io, k_ic, k_ci, k_bo, k_bi, k_bc,
        e_o, e_i, e_c,
        p_o0, p_c0,
        f_dyn, e_static
):
    params = Hsp90Params3State(
        k_OI=k_oi, k_IO=k_io, k_IC=k_ic, k_CI=k_ci,
        k_BO=k_bo, k_BI=k_bi, k_BC=k_bc,
        E_open=e_o, E_inter=e_i, E_closed=e_c,
        P_O0=p_o0, P_C0=p_c0
    )
    E_dyn = model_fret_3state(t_in, params)
    return f_dyn * E_dyn + (1.0 - f_dyn) * e_static

def fit_global_3state(
        t: np.ndarray,
        E_mat: np.ndarray,
        theta0: np.ndarray = None,
        n_starts: int = 5,
        n_jobs: int = 1
) -> Hsp90Fit3State:
    """
    Fit the 3-state + bleaching model plus static fraction to the ensemble mean.
    Uses a small multi-start strategy around theta0 to avoid bad local minima.

    Parameters
    ----------
    t : (T,) array
        Time grid.
    E_mat : (T, N) array
        FRET trajectories matrix.
    theta0 : (14,) array, optional
        Initial guess for parameters:
        [k_OI, k_IO, k_IC, k_CI, k_BO,
            k_BI, k_BC,
            E_open, E_inter, E_closed,
            P_O0, P_C0,
            f_dyn, E_static]
    n_starts : int
        Number of multi-starts for fitting (default: 5).
    Returns
    -------
    fit : Hsp90Fit3State
        Fitted model.
    Raises
    ------
    RuntimeError
        If no valid data is available for fitting.
    """
    row_valid = np.isfinite(E_mat).any(axis=1)
    t_fit = t[row_valid]
    E_mean = np.nanmean(E_mat[row_valid, :], axis=1)

    mask = np.isfinite(E_mean)
    t_fit = t_fit[mask]
    E_fit = E_mean[mask]

    # compute timewise SD to use as weights (sigma)
    E_std_all = np.nanstd(E_mat[row_valid, :], axis=1)
    sigma = E_std_all[mask]
    # avoid zeros
    sigma = np.where(np.isfinite(sigma) & (sigma > 1e-6), sigma, 1e-6)

    if t_fit.size == 0:
        raise RuntimeError("No valid data for fitting.")

    # 14 parameters: 7 rates + 3 FRET + 2 initials + 2 static
    if theta0 is None:
        theta0 = np.array([
            0.01, 0.01, 0.01, 0.01,  # k_OI, k_IO, k_IC, k_CI
            0.003, 0.006, 0.012,  # k_BO, k_BI, k_BC
            0.4, 0.5, 0.7,  # E_open, E_inter, E_closed (rough guesses)
            0.35, 0.55,  # P_O0, P_C0
            0.7, 0.18  # f_dyn, E_static
        ], dtype=float)

    lower = np.array([
        0.0, 0.0, 0.0, 0.0,  # rates
        0.0, 0.0, 0.0,  # bleaching
        0.0, 0.0, 0.0,  # FRET in [0,1]
        0.0, 0.0,  # P_O0, P_C0
        0.0, 0.0  # f_dyn, E_static
    ], dtype=float)

    upper = np.array([
        10.0, 10.0, 10.0, 10.0,
        2.0, 2.0, 2.0,  # bleaching slower-ish
        1.0, 1.0, 1.0,  # FRET ≤ 1
        1.0, 1.0,
        1.0, 1.0
    ], dtype=float)


    # ------------------------------------------------------------------
    # Multi-start around theta0 to avoid local minima
    # ------------------------------------------------------------------
    rng = np.random.default_rng(0)

    # Precompute all starting points deterministically
    start_configs: list[tuple[int, np.ndarray, str]] = []
    for s in range(n_starts):
        if s == 0:
            theta_start = theta0.copy()
            kind = "base"
        else:
            jitter = 1.0 + 0.3 * rng.normal(size=theta0.size)
            theta_start = theta0 * jitter
            theta_start = np.clip(theta_start, lower + 1e-8, upper - 1e-8)
            kind = "jitter"
        start_configs.append((s, theta_start, kind))

    def _single_start_worker(cfg: tuple[int, np.ndarray, str]) -> dict:
        """
        Run one curve_fit multi-start and return diagnostics.

        Parameters
        ----------
        cfg : tuple[int, ndarray, str]
            (start_index, theta_start, kind)
        Returns
        -------
        dict
            Dictionary with fit result and diagnostics.
        """
        s, theta_start, kind = cfg

        res_obj = cast(object, curve_fit(
            fret_wrapper_3s,
            t_fit, E_fit,
            p0=theta_start,
            bounds=(lower, upper),
            sigma=sigma,
            absolute_sigma=False,
            maxfev=20000,
        ))
        popt, pcov = cast(
            Tuple[NDArray[np.float64], NDArray[np.float64]],
            res_obj
        )
        # except Exception as e:
        #     return {
        #         "s": s,
        #         "kind": kind,
        #         "ok": False,
        #         "msg": str(e),
        #         "popt": None,
        #         "cost": np.inf,
        #         "rmse": np.inf,
        #     }

        # Evaluate fit quality
        E_model = fret_wrapper_3s(t_fit, *popt)
        mask_obj = np.isfinite(E_fit) & np.isfinite(E_model)
        if not np.any(mask_obj):
            return {
                "s": s,
                "kind": kind,
                "ok": False,
                "msg": "no valid points",
                "popt": None,
                "cost": np.inf,
                "rmse": np.inf,
            }

        resid = E_fit[mask_obj] - E_model[mask_obj]
        cost = float(np.mean(resid ** 2))
        rmse = float(np.sqrt(cost))

        return {
            "s": s,
            "kind": kind,
            "ok": True,
            "msg": "",
            "popt": popt,
            "cost": cost,
            "rmse": rmse,
        }

    results = Parallel(n_jobs=n_jobs, verbose=0)(
        delayed(_single_start_worker)(cfg) for cfg in start_configs
    )

    # Select best result
    best_popt: Optional[NDArray[np.float64]] = None
    best_cost = np.inf

    for res in results:
        s = res["s"]
        kind = res["kind"]
        if not res["ok"]:
            msg = res["msg"]
            logger.info(f"[fit_3state] multi-start {s + 1}/{n_starts} ({kind}) failed: {msg}")
            continue

        cost = res["cost"]
        rmse = res["rmse"]
        logger.info(
            f"[fit_3state] multi-start {s + 1}/{n_starts} ({kind}): RMSE = {rmse:.6f}"
        )

        if cost < best_cost:
            best_cost = cost
            best_popt = res["popt"]

    if best_popt is None:
        raise RuntimeError("fit_global_3state: all multi-start attempts failed.")

    logger.info(f"[fit_3state] selected solution with RMSE = {np.sqrt(best_cost):.6f}")
    popt = best_popt

    (k_oi, k_io, k_ic, k_ci,
     k_bo, k_bi, k_bc,
     e_o, e_i, e_c,
     p_o0, p_c0,
     f_dyn, e_static) = popt

    params = Hsp90Params3State(
        k_OI=float(k_oi),
        k_IO=float(k_io),
        k_IC=float(k_ic),
        k_CI=float(k_ci),
        k_BO=float(k_bo),
        k_BI=float(k_bi),
        k_BC=float(k_bc),
        E_open=float(e_o),
        E_inter=float(e_i),
        E_closed=float(e_c),
        P_O0=float(p_o0),
        P_C0=float(p_c0),
    )

    return Hsp90Fit3State(params=params,
                          f_dyn=float(f_dyn),
                          E_static=float(e_static))


# ----------------------------------------------------------------------
# Diagnostics / plotting
# ----------------------------------------------------------------------

def plot_ensemble_fit(t: np.ndarray,
                      E_mat: np.ndarray,
                      fit: Hsp90Fit3State,
                      outdir: Path) -> None:
    """
    Goodness-of-fit plot based on ensemble-averaged FRET.
    Plots mean observed FRET vs model FRET at each time point.

    Parameters
    ----------
    t : (T,) array
        Time grid.
    E_mat : (T, N) array
        FRET trajectories matrix.
    fit : Hsp90Fit3State
        Fitted model.
    outdir : Path
        Output directory to save the plot.
    Returns
    -------
    None
    -----------
    Compute ensemble mean and std dev at each time point.
    Evaluate model at those time points.
    Plot observed mean vs model prediction with y=x reference line.
    Compute and log RMSE and R^2.
    -----------
    """
    row_valid = np.isfinite(E_mat).any(axis=1)
    t_plot = t[row_valid]
    E_plot = E_mat[row_valid, :]

    if t_plot.size == 0:
        raise RuntimeError("No valid time points for ensemble fit plot.")

    E_mean = np.nanmean(E_plot, axis=1)
    E_std = np.nanstd(E_plot, axis=1)

    E_model = model_total_fret(t_plot, fit)

    mask = np.isfinite(E_mean) & np.isfinite(E_model)
    E_obs = E_mean[mask]
    E_mod = E_model[mask]

    if E_obs.size == 0:
        raise RuntimeError("No valid (mean, model) pairs for goodness-of-fit plot.")

    residuals = E_obs - E_mod
    rmse = np.sqrt(np.mean(residuals ** 2))
    ss_res = np.sum(residuals ** 2)
    ss_tot = np.sum((E_obs - E_obs.mean()) ** 2)
    r2 = 1.0 - ss_res / ss_tot if ss_tot > 0 else np.nan

    logger.info(f"Ensemble RMSE (mean data - model): {rmse:.6f}")
    logger.info(f"Ensemble R^2: {r2:.6f}")

    xmin = np.min(E_mod)
    xmax = np.max(E_mod)
    ymin = np.min(E_obs)
    ymax = np.max(E_obs)
    lo = min(xmin, ymin)
    hi = max(xmax, ymax)

    plt.figure(figsize=(8, 8))
    plt.scatter(E_mod, E_obs, s=20, alpha=0.8, label="Time points (ensemble mean)")
    plt.plot([lo, hi], [lo, hi], "k--", lw=1.5, label="y = x")

    plt.xlabel("Model FRET (ensemble)")
    plt.ylabel("Observed FRET (ensemble mean)")
    plt.title(
        "Goodness of fit (ensemble mean vs model)\n"
        f"RMSE = {rmse:.4f}, R^2 = {r2:.4f}"
    )
    plt.xlim(lo, hi)
    plt.ylim(lo, hi)
    plt.gca().set_aspect("equal", adjustable="box")
    plt.legend(loc="best")
    plt.tight_layout()

    plt.savefig(outdir / "ensemble_fit.png", dpi=300)
    plt.close()


def plot_hsp90_fit_time(
        t: np.ndarray,
        E_mat: np.ndarray,
        fit: Hsp90Fit3State,
        n_traces_overlay: int = 200,
        random_seed: int = 0,
        condition_key: str | None = None,
        outdir: Path = outdir
) -> None:
    """
    Plot Hsp90 3-state + bleaching + static fraction vs data across time.

    Parameters
    ----------
    t : (T,) array
        Time grid.
    E_mat : (T, N) array
        FRET trajectories matrix.
    fit : Hsp90Fit3State
        Fitted model.
    n_traces_overlay : int
        Number of individual trajectories to overlay (default: 200).
    random_seed : int
        Random seed for trajectory selection.
    outdir : Path
        Output directory to save the plot.
    Returns
    -------
    None
    -----------
    Compute ensemble mean and std dev at each time point.
    Evaluate model at those time points.
    Plot individual trajectories (subset), mean ± std dev, and model.
    -----------
    """
    row_valid = np.isfinite(E_mat).any(axis=1)
    t_plot = t[row_valid]
    E_plot = E_mat[row_valid, :]

    E_mean = np.nanmean(E_plot, axis=1)
    E_std = np.nanstd(E_plot, axis=1)

    E_model = model_total_fret(t_plot, fit)

    n_traj_total = E_plot.shape[1]
    if n_traces_overlay > 0 and n_traj_total > 0:
        n_traces_overlay = min(n_traces_overlay, n_traj_total)
        rng = np.random.default_rng(random_seed)
        idx = rng.choice(n_traj_total, size=n_traces_overlay, replace=False)
        E_subset = E_plot[:, idx]
    else:
        E_subset = None

    fig, ax = plt.subplots(figsize=(8, 8))

    if E_subset is not None:
        for j in range(E_subset.shape[1]):
            ax.plot(t_plot, E_subset[:, j], color="gray", alpha=0.05, lw=0.5)

    ax.plot(t_plot, E_mean, color="tab:blue", lw=2, label="Data Mean")
    ax.fill_between(
        t_plot,
        E_mean - E_std,
        E_mean + E_std,
        color="tab:blue",
        alpha=0.2,
        label="Data ±1 SD",
    )

    ax.plot(t_plot, E_model, color="tab:red", lw=2, label="Model")

    ax.set_xlabel("time (s)")
    ax.set_ylabel("FRET")
    ax.set_title(
        f"Model Fit - {condition_key}\n"
        f"{E_plot.shape[1]} trajectories, {len(t_plot)} time points"
    )
    ax.legend(loc="best")
    plt.tight_layout()

    plt.savefig(outdir / f"{condition_key}_fit.png", dpi=300)
    plt.close()


def _bootstrap_worker(
        t_sub: np.ndarray,
        E_sub: np.ndarray,
        seed: int
) -> Optional[dict]:
    """
    Internal worker for a single bootstrap resample and fit.

    Parameters
    ----------
    t_sub : (T_sub,) array
        Time grid for the subset condition.
    E_sub : (T_sub, N_sub) array
        FRET trajectories matrix for the subset condition.
    seed : int
        Random seed for resampling.
    Returns
    -------
    Optional[dict]
        Dictionary of fitted parameters if successful; None if fit failed.
    -----------
    1. Create a local RNG for this worker.
    2. Resample trajectories with replacement.
    3. Fit the resampled data.
    4. Pack results into a dictionary.
    -----------
    """
    # 1. Create a local RNG for this worker
    rng = np.random.default_rng(seed)

    # 2. Resample trajectories with replacement
    idx_boot = rng.integers(0, E_sub.shape[1], size=E_sub.shape[1])
    E_boot = E_sub[:, idx_boot]

    # 3. Fit the resampled data
    fit_b = fit_global_3state(t_sub, E_boot)

    # 4. Pack results
    if args.n_states == 3:
        fit_b = fit_global_3state(t_sub, E_boot)
        p = fit_b.params
        rec = dict(
            k_OI=p.k_OI, k_IO=p.k_IO, k_IC=p.k_IC, k_CI=p.k_CI,
            k_BO=p.k_BO, k_BI=p.k_BI, k_BC=p.k_BC,
            E_open=p.E_open, E_inter=p.E_inter, E_closed=p.E_closed,
            P_O0=p.P_O0, P_C0=p.P_C0,
            f_dyn=fit_b.f_dyn, E_static=fit_b.E_static,
        )
    else:
        fit_b = fit_global_2state(t_sub, E_boot)
        p = fit_b.params
        rec = dict(
            k_OC=p.k_OC, k_CO=p.k_CO,
            k_BO=p.k_BO, k_BC=p.k_BC,
            E_open=p.E_open, E_closed=p.E_closed,
            P_O0=p.P_O0,
            f_dyn=fit_b.f_dyn, E_static=fit_b.E_static,
        )
    return rec

def bootstrap_condition_params(
        t: np.ndarray,
        E_mat: np.ndarray,
        col_names: list[str],
        meta: pd.DataFrame,
        group_key: str,
        group_by: str = "condition",
        n_boot: int = 100,
        random_seed: int = 0,
        n_jobs: int = 4,
) -> pd.DataFrame:
    """
    Perform bootstrap resampling and fitting for a single condition.

    Parameters
    ----------
    t : (T,) array
        Global time grid.
    E_mat : (T, N) array
        Full combined FRET matrix.
    col_names : list of str
        Names of trajectory columns (same order as E_mat columns).
    meta : DataFrame
        Metadata DataFrame parsed from column names.
    group_key : str
        Condition key to bootstrap.
    group_by : {"condition", "construct", "exp_id"}
        How to group trajectories.
    n_boot : int
        Number of bootstrap replicates.
    random_seed : int
        Base random seed for resampling.
    n_jobs : int
        Number of parallel jobs.
    Returns
    -------
    pd.DataFrame
        DataFrame with one row per bootstrap replicate and fitted parameters.
    -----------
    1. Identify columns for the specified condition.
    2. Subset the data matrix for this condition.
    3. Perform parallel bootstrap fitting.
    4. Collect and return results.
    -----------
    """
    cols_subset = meta.loc[meta[group_by] == group_key, "col"].tolist()
    if not cols_subset:
        raise ValueError(f"No columns for {group_by}={group_key}")

    # Build submatrix for this condition (once)
    name_to_idx = {c: i for i, c in enumerate(col_names)}
    idx_all = [name_to_idx[c] for c in cols_subset if c in name_to_idx]
    E_full = E_mat[:, idx_all]

    row_valid = np.isfinite(E_full).any(axis=1)
    t_sub = t[row_valid]
    E_sub = E_full[row_valid, :]

    logger.info(f"Starting {n_boot} parallel bootstrap fits for {group_key}...")

    # --- PARALLEL BOOTSTRAP FITTING ---
    # We pass t_sub and E_sub (large arrays) once,
    # and iterate over the unique seeds.
    # verbose=5 will show a progress bar
    results = Parallel(n_jobs=n_jobs, verbose=0)(
        delayed(_bootstrap_worker)(t_sub, E_sub, random_seed + b)
        for b in range(n_boot)
    )
    # ------------------------

    # Unpack results, filtering out any failed (None) runs
    records = [res for res in results if res is not None]

    if not records:
        logger.info(f"Warning: All {n_boot} bootstrap fits failed for {group_key}.")
        return pd.DataFrame()

    return pd.DataFrame(records)


def plot_bootstrap_errorbars_all_conditions(
        boot_summary: pd.DataFrame,
        param: str,
        title_suffix: str = "",
        outdir: Path = outdir,
) -> None:
    """
    Plot bootstrap 95% confidence intervals for a given parameter across all conditions.

    Parameters
    ----------
    boot_summary : DataFrame
        DataFrame with bootstrap summary statistics.
    param : str
        Parameter name to plot (e.g., "k_OI", "E_open", etc.).
    title_suffix : str
        Suffix to add to the plot title.
    outdir : Path
        Output directory to save the plot.
    Returns
    -------
    None
    -----------
    1. Filter bootstrap summary for the specified parameter.
    2. Create error bar plot of mean ± 95% CI across conditions.
    -----------
    """
    df = boot_summary[boot_summary["param"] == param].copy()
    if df.empty:
        logger.info(f"No bootstrap summary for param={param}")
        return

    df = df.sort_values("group_key")
    x = np.arange(len(df))
    y = df["mean"].values
    yerr_lower = y - df["lo"].values
    yerr_upper = df["hi"].values - y
    yerr = np.vstack([yerr_lower, yerr_upper])

    plt.figure(figsize=(8, 4))
    plt.errorbar(
        x,
        y,
        yerr=yerr,
        fmt="o",
        capsize=4,
        linewidth=1.5,
    )
    plt.xticks(x, df["group_key"], rotation=45, ha="right")
    plt.ylabel(param)
    plt.title(f"Bootstrap 95% CI across conditions{title_suffix}")
    plt.tight_layout()

    plt.savefig(outdir / f"bootstrap_ci_{param}.png", dpi=300)
    plt.close()


def sobol_sensitivity_3state(
        t: np.ndarray,
        E_mat: np.ndarray,
        param_bounds: dict[str, tuple[float, float]],
        n_base_samples: int = 512,
        n_jobs: int = 1,
) -> dict:
    """
    Perform Sobol sensitivity analysis on the 3-state + bleaching + static model.

    Parameters
    ----------
    t : (T,) array
        Time grid.
    E_mat : (T, N) array
        FRET trajectories matrix.
    param_bounds : dict
        Dictionary mapping parameter names to (min, max) bounds for sampling.
        Example:
        {
            "k_OI": (0.001, 1.0),
            "k_IO": (0.001, 1.0),
            ...
        }
    n_base_samples : int
        Number of base samples for Saltelli sampling (default: 512).
    Returns
    -------
    dict
        Sobol sensitivity indices dictionary from SALib.
    -----------
    1. Prepare data: compute ensemble mean FRET for objective.
    2. Define SALib problem with parameter names and bounds.
    3. Generate Saltelli samples.
    4. Evaluate model for each sample and compute RMSE objective.
    5. Perform Sobol analysis on valid evaluations.
    -----------
    """

    # ---- 1. Prepare data: ensemble mean for objective -----------------
    row_valid = np.isfinite(E_mat).any(axis=1)
    t_fit = t[row_valid]
    E_mean = np.nanmean(E_mat[row_valid, :], axis=1)

    mask = np.isfinite(E_mean)
    t_fit = t_fit[mask]
    E_fit = E_mean[mask]

    if t_fit.size == 0:
        raise RuntimeError("No valid data for sensitivity analysis.")

    # ---- 2. Define SALib problem --------------------------------------
    # Order of parameters
    names = list(param_bounds.keys())
    bounds = [param_bounds[n] for n in names]

    problem = {
        "num_vars": len(names),
        "names": names,
        "bounds": bounds,
    }

    # ---- 3. Generate samples --------------------------------
    param_values = saltelli.sample(
        problem,
        N=n_base_samples,
        calc_second_order=True
    )

    # ---- 4. Evaluate model for each sample -------

    def _eval_one(theta: np.ndarray) -> float:
        # Map sample vector -> parameter dict
        p_dict = dict(zip(names, theta))

        # Defaults if not varied
        k_OI = p_dict.get("k_OI", 0.08)
        k_IO = p_dict.get("k_IO", 0.01)
        k_IC = p_dict.get("k_IC", 0.08)
        k_CI = p_dict.get("k_CI", 0.01)

        k_BO = p_dict.get("k_BO", 0.003)
        k_BI = p_dict.get("k_BI", 0.006)
        k_BC = p_dict.get("k_BC", 0.012)

        E_open = p_dict.get("E_open", 0.35)
        E_inter = p_dict.get("E_inter", 0.55)
        E_closed = p_dict.get("E_closed", 0.75)

        P_O0 = p_dict.get("P_O0", 0.4)
        P_C0 = p_dict.get("P_C0", 0.5)

        f_dyn = p_dict.get("f_dyn", 0.7)
        E_static = p_dict.get("E_static", 0.2)

        params = Hsp90Params3State(
            k_OI=k_OI, k_IO=k_IO, k_IC=k_IC, k_CI=k_CI,
            k_BO=k_BO, k_BI=k_BI, k_BC=k_BC,
            E_open=E_open, E_inter=E_inter, E_closed=E_closed,
            P_O0=P_O0, P_C0=P_C0,
        )

        fit = Hsp90Fit3State(params=params, f_dyn=f_dyn, E_static=E_static)

        try:
            E_model = model_total_fret(t_fit, fit)
            mask_obj = np.isfinite(E_model) & np.isfinite(E_fit)
            if not np.any(mask_obj):
                return np.nan

            r = E_fit[mask_obj] - E_model[mask_obj]
            rmse = np.sqrt(np.mean(r ** 2))
            return float(rmse)
        except Exception:
            return np.nan

    Y = np.array(
        Parallel(n_jobs=n_jobs, verbose=0)(
            delayed(_eval_one)(theta) for theta in param_values
        ),
        dtype=float,
    )

    # Clean up NaNs
    valid_mask = np.isfinite(Y)
    if not np.any(valid_mask):
        raise RuntimeError("All Sobol evaluations failed or returned NaN.")

    Y_valid = Y[valid_mask]
    X_valid = param_values[valid_mask, :]
    n_valid = X_valid.shape[0]

    logger.info(f"Sensitivity analysis: {n_valid}/{len(Y)} evaluations valid.")

    # Re-run analysis only on valid samples
    Si = sobol.analyze(
        problem,
        Y_valid,
        calc_second_order=True,
        print_to_console=False,
        parallel=True,
        n_processors=os.cpu_count() if n_jobs == -1 else n_jobs,
    )

    # Attach parameter names for convenience
    Si["names"] = names
    return Si


def sobol_sensitivity_for_group(
        t: np.ndarray,
        E_mat: np.ndarray,
        col_names: list[str],
        meta: pd.DataFrame,
        group_key: str,
        group_by: str,
        param_bounds: dict[str, tuple[float, float]],
        n_base_samples: int = 512,
        n_jobs: int = 1
) -> pd.DataFrame:
    """
    Perform Sobol sensitivity analysis for a specific group/condition.

    Parameters
    ----------
    t : (T,) array
        Global time grid.
    E_mat : (T, N) array
        Full combined FRET matrix.
    col_names : list of str
        Names of trajectory columns (same order as E_mat columns).
    meta : DataFrame
        Metadata DataFrame parsed from column names.
    group_key : str
        Condition key to analyze.
    group_by : {"condition", "construct", "exp_id"}
        How to group trajectories.
    param_bounds : dict
        Dictionary mapping parameter names to (min, max) bounds for sampling.
    n_base_samples : int
        Number of base samples for Saltelli sampling (default: 512).
    Returns
    -------
    pd.DataFrame
        DataFrame with Sobol sensitivity indices for the group.
    -----------
    1. Identify columns for the specified group.
    2. Subset the data matrix for this group.
    3. Perform Sobol sensitivity analysis.
    4. Pack results into a DataFrame.
    -----------
    """
    cols_subset = meta.loc[meta[group_by] == group_key, "col"].tolist()
    if not cols_subset:
        raise ValueError(f"No columns found for {group_by}={group_key!r}")

    # subset matrix
    t_sub, E_sub = subset_matrix_by_columns(t, E_mat, col_names, cols_subset)

    logger.info(f"Running Sobol SA for {group_by}={group_key}, "
                f"{E_sub.shape[1]} traj, {E_sub.shape[0]} time points")

    Si = sobol_sensitivity_3state(
        t_sub,
        E_sub,
        param_bounds=param_bounds,
        n_base_samples=n_base_samples,
        n_jobs=n_jobs
    )

    # pack into a nice DataFrame
    df_sa = pd.DataFrame({
        "param": Si["names"],
        "S1": Si["S1"],
        "S1_conf": Si["S1_conf"],
        "ST": Si["ST"],
        "ST_conf": Si["ST_conf"],
    })
    df_sa.insert(0, "group_key", group_key)
    df_sa.insert(0, "group_by", group_by)

    return df_sa


def plot_param_vs_condition(summary_df, param, outdir):
    """
    Plot a fitted parameter vs condition.

    Parameters
    ----------
    summary_df : DataFrame
        DataFrame with fitted parameters per condition.
    param : str
        Parameter name to plot.
    outdir : Path
        Output directory to save the plot.
    Returns
    -------
    None
    -----------
    """
    if summary_df is None or summary_df.empty:
        logger.info(f"Skip plot {param}: empty summary_df")
        return
    if param not in summary_df.columns:
        logger.info(f"Skip plot {param}: column not in summary_df")
        return

    plt.figure(figsize=(8, 8))
    plt.plot(summary_df["group_key"], summary_df[param], "o-")
    plt.xticks(rotation=90)
    plt.ylabel(param)
    plt.tight_layout()

    plt.savefig(outdir / f"{param}_vs_condition.png", dpi=300)
    plt.close()


def summarize_bootstrap(boot_df, name):
    """
    Summarize bootstrap results for a given parameter.

    Parameters
    ----------
    boot_df : DataFrame
        DataFrame with bootstrap results.
    name : str
        Parameter name to summarize.
    Returns
    -------
    tuple
        (mean, 2.5th percentile, 97.5th percentile)
    -------
    """
    vals = boot_df[name].values
    mean = np.nanmean(vals)
    lo, hi = np.nanpercentile(vals, [2.5, 97.5])
    return mean, lo, hi


def plot_bootstrap_compare(boot_A, boot_B, param, label_A, label_B, outdir):
    """
    Plot comparison of bootstrap distributions for a given parameter.

    Parameters
    ----------
    boot_A : DataFrame
        Bootstrap results for condition A.
    boot_B : DataFrame
        Bootstrap results for condition B.
    param : str
        Parameter name to plot.
    label_A : str
        Label for condition A.
    label_B : str
        Label for condition B.
    outdir : Path
        Output directory to save the plot.
    Returns
    -------
    None
    -------
    """
    A_vals = boot_A[param].values
    B_vals = boot_B[param].values

    plt.figure(figsize=(8, 8))
    plt.hist(A_vals, bins=30, alpha=0.5, density=True, label=label_A)
    plt.hist(B_vals, bins=30, alpha=0.5, density=True, label=label_B)
    plt.xlabel(param)
    plt.ylabel("density")
    plt.legend()
    plt.tight_layout()

    plt.savefig(outdir / f"bootstrap_compare_{param}.png", dpi=300)
    plt.close()


def plot_residuals_over_time(t, E_mat, fit, outdir):
    """
    Plot residuals (mean data - model) over time for ensemble fit.

    Parameters
    ----------
    t : (T,) array
        Global time grid.
    E_mat : (T, N) array
        Full combined FRET matrix.
    fit : Hsp90Fit3State
        Fitted model to use for residual calculation.
    outdir : Path
        Output directory to save the plot.

    Returns
    -------
    None
    """
    row_valid = np.isfinite(E_mat).any(axis=1)
    t_plot = t[row_valid]
    E_mean = np.nanmean(E_mat[row_valid, :], axis=1)
    E_model = model_total_fret(t_plot, fit)
    r = E_mean - E_model
    plt.figure(figsize=(8, 8))
    plt.plot(t_plot, r, lw=1)
    plt.axhline(0, ls="--", c="k", lw=1)
    plt.xlabel("time (s)")
    plt.ylabel("residual (mean - model)")
    plt.title("Ensemble residuals over time")
    plt.tight_layout()

    plt.savefig(outdir / "ensemble_residuals_time.png", dpi=300)
    plt.close()


# ----------------------------------------------------------------------
# Main
# ----------------------------------------------------------------------
def main():
    logger.info(f"[bold green]Using outdir:[/bold green] {outdir.resolve()}")

    combined_path = Path("data/timeseries/fret_matrix.csv")
    t, E_mat, col_names = load_combined_matrix(combined_path)

    logger.info(f"Loaded combined matrix: {E_mat.shape[0]} time points, {E_mat.shape[1]} trajectories")

    # Global fit
    logger.info("[bold magenta]\n=== Global fit ===[/bold magenta]")
    if args.n_states == 3:
        fit_hat = fit_global_3state(t, E_mat, n_starts=args.multistarts, n_jobs=args.cores)
        df_fit_hat = fit_to_df(fit_hat)
    else:
        fit_hat = fit_global_2state(t, E_mat, n_starts=args.multistarts, n_jobs=args.cores)
        df_fit_hat = fit_to_df_2state(fit_hat)

    log_df(df_fit_hat, title="Best-fit parameters (global)")

    # Global diagnostics
    plot_ensemble_fit(t, E_mat, fit_hat, outdir)
    plot_hsp90_fit_time(t, E_mat, fit_hat, n_traces_overlay=200, condition_key="Global Fit", outdir=outdir)
    plot_residuals_over_time(t, E_mat, fit_hat, outdir)

    # per-condition fits
    # 1) Per coverslip/day (construct + exp_id)
    logger.info("[bold magenta]\n=== Per-condition fits (construct + exp_id) ===[/bold magenta]")
    summary_cond, fits_cond = fit_all_conditions(
        t,
        E_mat,
        col_names,
        group_by="condition",  # "<construct>_<exp_id>"
        do_plots=True,
        max_overlay_traces=100,
        n_starts=args.multistarts,
        n_jobs=args.cores,
    )
    if not summary_cond.empty:
        log_df(
            summary_cond.sort_values("group_key"),
            title="Condition-level summary"
        )
    else:
        logger.info("[bold yellow]No condition-level fits were produced.[/bold yellow]")

    if not summary_cond.empty:
        (outdir / "summary_conditions.csv").write_text(
            summary_cond.to_csv(index=False)
        )

    # 2) Per construct (pooling all days), if you want
    logger.info("[bold magenta]\n=== Per-construct fits (pool all exp_id for each construct) ===[/bold magenta]")
    summary_constr, fits_constr = fit_all_conditions(
        t,
        E_mat,
        col_names,
        group_by="construct",
        do_plots=True,
        max_overlay_traces=100,
        n_starts=args.multistarts,
        n_jobs=args.cores,
    )
    if not summary_constr.empty:
        log_df(
            summary_constr.sort_values("group_key"),
            title="Construct-level summary"
        )
    else:
        logger.info("[bold yellow]No construct-level fits were produced.[/bold yellow]")

    if not summary_constr.empty:
        (outdir / "summary_constructs.csv").write_text(
            summary_constr.to_csv(index=False)
        )

    # parameter-vs-condition plots
    if not summary_cond.empty:
        if args.n_states == 3:
            plot_param_vs_condition(summary_cond, "k_OI", outdir)
            plot_param_vs_condition(summary_cond, "f_dyn", outdir)
            plot_param_vs_condition(summary_cond, "E_closed", outdir)
        else:
            plot_param_vs_condition(summary_cond, "k_OC", outdir)
            plot_param_vs_condition(summary_cond, "f_dyn", outdir)
            plot_param_vs_condition(summary_cond, "E_closed", outdir)

    # meta = parse_column_metadata(col_names)
    #
    # param_bounds = {
    #     "k_OI": (0.0, 10.0),
    #     "k_IO": (0.0, 10.0),
    #     "k_IC": (0.0, 10.0),
    #     "k_CI": (0.0, 10.0),
    #
    #     "k_BO": (0.0, 2.0),
    #     "k_BI": (0.0, 2.0),
    #     "k_BC": (0.0, 2.0),
    #
    #     "E_open": (0.0, 1.0),
    #     "E_inter": (0.0, 1.0),
    #     "E_closed": (0.0, 1.0),
    #
    #     "P_O0": (0.0, 1.0),
    #     "P_C0": (0.0, 1.0),
    #
    #     "f_dyn": (0.0, 1.0),
    #     "E_static": (0.0, 1.0),
    # }
    #
    # Si = sobol_sensitivity_3state(
    #     t,
    #     E_mat,
    #     param_bounds,
    #     n_base_samples=512,
    #     n_jobs=args.cores
    # )
    #
    # # Inspect first-order and total-order indices as a table
    # df_sobol = pd.DataFrame({
    #     "param": Si["names"],
    #     "S1": Si["S1"],
    #     "S1_conf": Si["S1_conf"],
    #     "ST": Si["ST"],
    #     "ST_conf": Si["ST_conf"],
    # })
    # log_df(df_sobol, title="Global Sobol sensitivity (all trajectories)")
    #
    # # === Sobol sensitivity per condition ==============================
    # sa_cond_list: list[pd.DataFrame] = []
    #
    # for key in summary_cond["group_key"]:
    #     try:
    #         df_sa = sobol_sensitivity_for_group(
    #             t=t,
    #             E_mat=E_mat,
    #             col_names=col_names,
    #             meta=meta,
    #             group_key=key,
    #             group_by="condition",
    #             param_bounds=param_bounds,
    #             n_base_samples=256,
    #             n_jobs=args.cores
    #         )
    #         sa_cond_list.append(df_sa)
    #     except Exception as e:
    #         logger.info(f"[condition SA] Skipped {key}: {e}")
    #
    # if sa_cond_list:
    #     sa_cond = pd.concat(sa_cond_list, ignore_index=True)
    #     log_df(sa_cond, title="Sobol sensitivity – per condition")
    #     # Optionally save:
    #     sa_cond.to_csv(outdir / "sobol_condition.csv", index=False)
    # else:
    #     logger.info("No per-condition Sobol results.")
    #
    # # === Sobol sensitivity per construct ==============================
    # sa_constr_list: list[pd.DataFrame] = []
    #
    # for key in summary_constr["group_key"]:
    #     try:
    #         df_sa = sobol_sensitivity_for_group(
    #             t=t,
    #             E_mat=E_mat,
    #             col_names=col_names,
    #             meta=meta,
    #             group_key=key,
    #             group_by="construct",
    #             param_bounds=param_bounds,
    #             n_base_samples=256,
    #             n_jobs=args.cores
    #         )
    #         sa_constr_list.append(df_sa)
    #     except Exception as e:
    #         logger.info(f"[construct SA] Skipped {key}: {e}")
    #
    # if sa_constr_list:
    #     sa_constr = pd.concat(sa_constr_list, ignore_index=True)
    #     log_df(sa_constr, title="Sobol sensitivity – per construct")
    #     # Optionally save:
    #     sa_constr.to_csv(outdir / "sobol_construct.csv", index=False)
    # else:
    #     logger.info("No per-construct Sobol results.")
    #
    # # === Bootstrap for ALL conditions ================================
    # logger.info("\n=== Bootstrap parameter uncertainty per condition ===")
    #
    # boot_records: list[dict] = []
    # boot_raw: dict[str, pd.DataFrame] = {}
    #
    # # choose which parameters you care about
    # params_of_interest = ["k_OI", "k_IC", "f_dyn", "E_closed"]
    #
    # for key in summary_cond["group_key"]:
    #     logger.info(f"\n[bootstrap] condition={key}")
    #     try:
    #         boot_df = bootstrap_condition_params(
    #             t=t,
    #             E_mat=E_mat,
    #             col_names=col_names,
    #             meta=meta,
    #             group_key=key,
    #             group_by="condition",
    #             n_boot=10,
    #             random_seed=0,
    #             n_jobs=args.cores,
    #         )
    #     except Exception as e:
    #         logger.info(f"  bootstrap failed for {key}: {e}")
    #         continue
    #
    #     if boot_df.empty:
    #         logger.info(f"  no successful bootstrap fits for {key}")
    #         continue
    #
    #     boot_raw[key] = boot_df
    #
    #     # summarize mean + 95% CI per parameter
    #     for p in params_of_interest:
    #         if p not in boot_df.columns:
    #             continue
    #         m, lo, hi = summarize_bootstrap(boot_df, p)
    #         boot_records.append(
    #             dict(
    #                 group_key=key,
    #                 param=p,
    #                 mean=m,
    #                 lo=lo,
    #                 hi=hi,
    #                 n_boot=len(boot_df),
    #             )
    #         )
    #
    # if not boot_records:
    #     logger.info("No bootstrap summaries computed.")
    # else:
    #     boot_summary = pd.DataFrame(boot_records)
    #     log_df(boot_summary, title="Bootstrap summary (mean ± 95% CI per condition/param)")
    #
    #     if not boot_summary.empty:
    #         (outdir / "bootstrap_summary_conditions.csv").write_text(
    #             boot_summary.to_csv(index=False)
    #         )
    #
    #     # Plot all conditions together for each parameter of interest
    #     for p in params_of_interest:
    #         plot_bootstrap_errorbars_all_conditions(
    #             boot_summary,
    #             param=p,
    #             title_suffix=" (condition-level)",
    #         )
    #
    # # === Bootstrap for ALL constructs ================================
    # logger.info("\n=== Bootstrap parameter uncertainty per construct ===")
    #
    # boot_records_constr: list[dict] = []
    # boot_raw_constr: dict[str, pd.DataFrame] = {}
    #
    # for key in summary_constr["group_key"]:
    #     logger.info(f"\n[bootstrap] construct={key}")
    #     try:
    #         boot_df = bootstrap_condition_params(
    #             t=t,
    #             E_mat=E_mat,
    #             col_names=col_names,
    #             meta=meta,
    #             group_key=key,
    #             group_by="construct",
    #             n_boot=args.bootstraps,
    #             random_seed=0,
    #             n_jobs=args.cores,
    #         )
    #     except Exception as e:
    #         logger.info(f"  bootstrap failed for construct {key}: {e}")
    #         continue
    #
    #     if boot_df.empty:
    #         logger.info(f"  no successful bootstrap fits for construct {key}")
    #         continue
    #
    #     boot_raw_constr[key] = boot_df
    #
    #     for p in params_of_interest:
    #         if p not in boot_df.columns:
    #             continue
    #         m, lo, hi = summarize_bootstrap(boot_df, p)
    #         boot_records_constr.append(
    #             dict(
    #                 group_type="construct",
    #                 group_key=key,
    #                 param=p,
    #                 mean=m,
    #                 lo=lo,
    #                 hi=hi,
    #                 n_boot=len(boot_df),
    #             )
    #         )
    #
    # if not boot_records_constr:
    #     logger.info("No bootstrap summaries computed for constructs.")
    # else:
    #     boot_summary_constr = pd.DataFrame(boot_records_constr)
    #     log_df(boot_summary_constr, title="Bootstrap summary (mean ± 95% CI per construct/param)")
    #
    #     if not boot_summary_constr.empty:
    #         (outdir / "bootstrap_summary_constructs.csv").write_text(
    #             boot_summary_constr.to_csv(index=False)
    #         )
    #
    #     for p in params_of_interest:
    #         plot_bootstrap_errorbars_all_conditions(
    #             boot_summary_constr,
    #             param=p,
    #             title_suffix=" (construct-level)",
    #         )


if __name__ == "__main__":
    main()
