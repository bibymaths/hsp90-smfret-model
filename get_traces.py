# SPDX-License-Identifier: BSD-3-Clause
# SPDX-FileCopyrightText: 2025 Abhinav Mishra

"""
Figures and visual outputs generated by this script are licensed under:
SPDX-License-Identifier: CC-BY-4.0
"""

# BSD 3-Clause License
#
# Copyright (c) 2025, Abhinav Mishra
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:
#
# 1. Redistributions of source code must retain the above copyright notice, this
#    list of conditions and the following disclaimer.
#
# 2. Redistributions in binary form must reproduce the above copyright notice,
#    this list of conditions and the following disclaimer in the documentation
#    and/or other materials provided with the distribution.
#
# 3. Neither the name of the copyright holder nor the names of its
#    contributors may be used to endorse or promote products derived from
#    this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

# HDF5 FRET Tracking Data Processor
# =================================
#
# This script processes single-molecule FRET tracking data stored in HDF5 format.
# It is designed to inspect raw data, filter and export individual particle
# trajectories, and combine them into a unified time-series matrix.

import argparse
import logging
import os
import warnings
from concurrent.futures import ProcessPoolExecutor, as_completed
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from scipy.interpolate import CubicSpline

# === Logger Setup ===
logging.basicConfig(level=logging.INFO, format="%(message)s")
logger = logging.getLogger(__name__)

warnings.filterwarnings("ignore", category=UserWarning, module="pandas")

# === Global configuration (overridden by CLI in main) ===
data_dir = Path("data/Hugel_2025")  # adjusted via --data-dir
export_dir = Path("data/timeseries")  # adjusted via --export-dir
key = "/tracks/Data"  # default key
# Paper: 70ms integration + 20ms readout = 180ms per full ALEX cycle
frame_interval = 0.18  # seconds per ALEX cycle (if donor_frame counts cycles)
fret_max = 1.0  # max FRET efficiency to consider
fret_min = 0.0  # min FRET efficiency to consider
USE_INTERPOLATION = False  # Set to False - disable cubic splines/filling
# --- correction factors (Based on Anandamurugan et al. 2026, Suppl. Table 6) ---
# --- choose per acquisition mode; defaults here assume HILO) ---
alpha = 0.17  # donor leakage into acceptor channel
delta = 0.12  # direct excitation contribution to DA from Aex channel
gamma = 0.8   # detection efficiency factor

beta = 1.0    # stoichiometry scaling for AA (HILO often ~1); keep explicit

# background offsets (estimated from post-bleach segment; default 0 if not available)
bg_DD = 0.0
bg_DA = 0.0
bg_AA = 0.0

# Stoichiometry window (typical paper QC; override by CLI)
S_min = 0.35
S_max = 0.60

# Pre-bleach trimming parameters
BLEACH_MIN_FRAC = 0.20  # acceptor intensity below 20% of early baseline -> bleached
BLEACH_CONSEC = 3  # must stay low for >= 3 consecutive points
BASELINE_N = 10  # use first N points to define baseline


def interpolate_trace(
        time_grid: np.ndarray,
        t_trace: np.ndarray,
        E_trace: np.ndarray,
        interpolate: bool = True,
) -> np.ndarray:
    """
    Interpolate a single FRET trace onto a common time grid using cubic splines.

    Handles edge cases such as missing data (NaNs), single-point traces,
    and short traces where only linear interpolation is feasible.
    """
    t_trace = np.asarray(t_trace, float)
    E_trace = np.asarray(E_trace, float)

    mask = np.isfinite(t_trace) & np.isfinite(E_trace)
    t_clean = t_trace[mask]
    E_clean = E_trace[mask]

    if t_clean.size == 0:
        return np.full_like(time_grid, np.nan, dtype=float)
    if t_clean.size == 1:
        y = np.full_like(time_grid, np.nan, dtype=float)
        idx = np.argmin(np.abs(time_grid - t_clean[0]))
        y[idx] = E_clean[0]
        return y

    t_unique, idx_unique = np.unique(t_clean, return_index=True)
    E_unique = E_clean[idx_unique]

    if not interpolate:
        y = np.full_like(time_grid, np.nan, dtype=float)
        if len(time_grid) > 1:
            dt = time_grid[1] - time_grid[0]
            idx = np.rint(t_unique / dt).astype(np.int64)
            valid = (idx >= 0) & (idx < len(time_grid))
            y[idx[valid]] = E_unique[valid]
        elif len(time_grid) == 1 and t_unique.size > 0:
            if np.abs(t_unique[0] - time_grid[0]) < frame_interval / 2:
                y[0] = E_unique[0]
        return y

    if t_unique.size < 2:
        return np.full_like(time_grid, np.nan, dtype=float)

    if t_unique.size == 2:
        y = np.interp(time_grid, t_unique, E_unique, left=np.nan, right=np.nan)
        return y

    try:
        cs = CubicSpline(t_unique, E_unique, extrapolate=False)
        y = cs(time_grid)
    except Exception:
        y = np.interp(time_grid, t_unique, E_unique, left=np.nan, right=np.nan)

    outside = (time_grid < t_unique.min()) | (time_grid > t_unique.max())
    y[outside] = np.nan
    y[(y < fret_min) | (y > fret_max)] = np.nan
    return y


def _sample_particles(df: pd.DataFrame, n: int = 20, seed: int = 0) -> list[int]:
    pids = df["fret_particle"].dropna().unique()
    if len(pids) == 0:
        return []
    rng = np.random.default_rng(seed)
    n = min(n, len(pids))
    return rng.choice(pids, size=n, replace=False).tolist()


def plot_file_qc(df: pd.DataFrame,
                 outdir: Path,
                 tag: str,
                 n_particles: int = 20) -> None:
    outdir.mkdir(parents=True, exist_ok=True)

    dex = df[df["fret_exc_type"] == "d"].copy()
    aex = df[df["fret_exc_type"] == "a"].copy()

    pids = _sample_particles(df, n=n_particles, seed=0)

    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    ax1, ax2, ax3, ax4 = axes.ravel()

    for pid in pids:
        tr = dex[dex["fret_particle"] == pid]
        if len(tr) == 0:
            continue
        ax1.plot(tr["time_s"], tr["E"], linewidth=1, alpha=0.7)
    ax1.set_title("Dex: E(t) sampled particles")
    ax1.set_xlabel("time (s)")
    ax1.set_ylabel("E")

    s = df["S"].to_numpy()
    s = s[np.isfinite(s)]
    if s.size:
        ax2.hist(s, bins=60)
    ax2.set_title("Stoichiometry S distribution")
    ax2.set_xlabel("S")
    ax2.set_ylabel("count")
    ax2.axvline(S_min, linestyle="--")
    ax2.axvline(S_max, linestyle="--")

    e = dex["E"].to_numpy()
    e = e[np.isfinite(e)]
    if e.size:
        ax3.hist(e, bins=60)
    ax3.set_title("Dex: E distribution")
    ax3.set_xlabel("E")
    ax3.set_ylabel("count")

    for pid in pids:
        tr = aex[aex["fret_particle"] == pid]
        if len(tr) == 0:
            continue
        ax4.plot(tr["time_s"], tr["fret_f_aa"], linewidth=1, alpha=0.7)
    ax4.set_title("Aex: AA(t) sampled particles")
    ax4.set_xlabel("time (s)")
    ax4.set_ylabel("AA")

    fig.suptitle(tag)
    fig.tight_layout()
    fig.savefig(outdir / f"{tag}_QC.png", dpi=300)
    plt.close(fig)


def plot_construct_summary(export_dir: Path, outdir: Path) -> None:
    outdir.mkdir(parents=True, exist_ok=True)

    csvs = [p for p in export_dir.glob("*.csv") if p.name != "fret_matrix.csv"]
    if not csvs:
        return

    groups = {}
    for p in csvs:
        stem = p.stem
        parts = stem.split("-")
        construct = parts[2].split(".")[0] if len(parts) >= 3 else "unknown"
        groups.setdefault(construct, []).append(p)

    for construct, files in groups.items():
        all_df = []
        for f in files:
            d = pd.read_csv(f)
            d["src"] = f.name
            all_df.append(d)
        big = pd.concat(all_df, ignore_index=True)

        agg = big.groupby("time_s")["E"].agg(
            median="median",
            q25=lambda x: np.nanquantile(x, 0.25),
            q75=lambda x: np.nanquantile(x, 0.75),
            n="count",
        ).reset_index()

        fig, ax = plt.subplots(figsize=(10, 5))
        ax.plot(agg["time_s"], agg["median"])
        ax.fill_between(agg["time_s"], agg["q25"], agg["q75"], alpha=0.3)
        ax.set_title(f"{construct}: median E(t) with IQR band")
        ax.set_xlabel("time (s)")
        ax.set_ylabel("E")
        fig.tight_layout()
        fig.savefig(outdir / f"{construct}_summary.png", dpi=300)
        plt.close(fig)


def _pick_first_present(df: pd.DataFrame, candidates: list[str]) -> str | None:
    for c in candidates:
        if c in df.columns:
            return c
    return None


def compute_corrected_E_S(df: pd.DataFrame, alpha: float, delta: float, gamma: float,
                          bg_DD: float = 0.0, bg_DA: float = 0.0, bg_AA: float = 0.0
                          ) -> pd.DataFrame:
    """
    Compute Dex-only E, and ALEX stoichiometry S by pairing Dex rows with nearest Aex AA.
    Requires: fret_f_dd, fret_f_da, fret_f_aa, fret_exc_type, fret_particle, donor_frame (+ acceptor_frame if present).
    """

    required = {"fret_f_dd", "fret_f_da", "fret_f_aa", "fret_exc_type", "fret_particle", "donor_frame"}
    missing = required - set(df.columns)
    if missing:
        raise ValueError(f"Missing columns: {missing}")

    df = df.copy()
    if "acceptor_frame" in df.columns:
        df["frame"] = np.where(df["fret_exc_type"] == "a",
                               df["acceptor_frame"].astype(float),
                               df["donor_frame"].astype(float))
    else:
        df["frame"] = df["donor_frame"].astype(float)

    dex = df[df["fret_exc_type"] == "d"].copy()
    aex = df[df["fret_exc_type"] == "a"].copy()

    DD = dex["fret_f_dd"].astype(float).to_numpy()
    DA = dex["fret_f_da"].astype(float).to_numpy()

    # DA_corr = DA - alpha * DD
    # DD_corr = DD

    # subtract backgrounds first (these should be estimated from post-bleach if possible)
    DD0 = DD - bg_DD
    DA0 = DA - bg_DA

    DA_corr = DA0 - alpha * DD0
    DD_corr = DD0

    DA_corr[DA_corr <= 0] = np.nan
    DD_corr[DD_corr <= 0] = np.nan

    dex_small = dex[["fret_particle", "frame"]].copy()
    aex_small = aex[["fret_particle", "frame", "fret_f_aa"]].copy()
    aex_small["AA"] = aex_small["fret_f_aa"].astype(float)
    aex_small = aex_small.drop(columns=["fret_f_aa"])

    dex_small = dex_small.sort_values(["frame", "fret_particle"]).reset_index(drop=True)
    aex_small = aex_small.sort_values(["frame", "fret_particle"]).reset_index(drop=True)

    dex_small = dex_small[np.isfinite(dex_small["frame"])]
    aex_small = aex_small[np.isfinite(aex_small["frame"])]

    # Prefer exact pairing by cycle index (robust if donor_frame is the ALEX cycle counter)
    paired = pd.merge(
        dex_small,
        aex_small,
        on=["fret_particle", "frame"],
        how="left",
    )

    # If too many missing AA after exact merge, fall back to nearest with tight tolerance
    missing_frac = paired["AA"].isna().mean()
    if missing_frac > 0.10:
        paired = pd.merge_asof(
            dex_small.sort_values(["fret_particle", "frame"]),
            aex_small.sort_values(["fret_particle", "frame"]),
            on="frame",
            by="fret_particle",
            direction="nearest",
            tolerance=0.25,  # << much tighter than 1.0
            allow_exact_matches=True,
        )

    # AA = paired["AA"].to_numpy()
    # AA[AA <= 0] = np.nan
    #
    # if delta != 0.0:
    #     DA_corr = DA_corr - delta * AA
    #     DA_corr[DA_corr <= 0] = np.nan
    #
    # denom_E = DA_corr + gamma * DD_corr
    # E = np.where(denom_E > 0, DA_corr / denom_E, np.nan)
    #
    # denom_S = DA_corr + gamma * DD_corr + AA
    # S = np.where(denom_S > 0, (DA_corr + gamma * DD_corr) / denom_S, np.nan)

    AA = paired["AA"].to_numpy()
    AA = AA - bg_AA
    AA[AA <= 0] = np.nan

    if delta != 0.0:
        DA_corr = DA_corr - delta * AA
        DA_corr[DA_corr <= 0] = np.nan

    denom_E = DA_corr + gamma * DD_corr
    E = np.where(denom_E > 0, DA_corr / denom_E, np.nan)

    # beta belongs here (explicit)
    denom_S = DA_corr + gamma * DD_corr + beta * AA
    S = np.where(denom_S > 0, (DA_corr + gamma * DD_corr) / denom_S, np.nan)

    dex["E"] = E
    dex["S"] = S

    out = df.copy()
    out["E"] = np.nan
    out["S"] = np.nan
    out.loc[dex.index, "E"] = dex["E"].values
    out.loc[dex.index, "S"] = dex["S"].values

    return out

def detect_first_bleach_time(traj: pd.DataFrame, baseline_n: int, min_frac: float, consec: int) -> float | None:
    """
    Return time_s of the first bleaching event (donor OR acceptor), using DD and AA.
    """
    # Work on Dex DD and Aex AA separately
    dex = traj[traj["fret_exc_type"] == "d"].copy()
    aex = traj[traj["fret_exc_type"] == "a"].copy()

    if dex.shape[0] < max(baseline_n, consec + 1) or aex.shape[0] < max(baseline_n, consec + 1):
        return None

    dd = dex["fret_f_dd"].astype(float).to_numpy()
    aa = aex["fret_f_aa"].astype(float).to_numpy()

    dd_base = np.nanmedian(dd[:baseline_n])
    aa_base = np.nanmedian(aa[:baseline_n])
    if not np.isfinite(dd_base) or dd_base <= 0 or not np.isfinite(aa_base) or aa_base <= 0:
        return None

    dd_thr = min_frac * dd_base
    aa_thr = min_frac * aa_base

    def _first_run_below(x, thr):
        run = 0
        for i in range(len(x)):
            run = run + 1 if (np.isfinite(x[i]) and x[i] < thr) else 0
            if run >= consec:
                return i - consec + 1
        return None

    i_dd = _first_run_below(dd, dd_thr)
    i_aa = _first_run_below(aa, aa_thr)

    t_dd = float(dex.iloc[i_dd]["time_s"]) if i_dd is not None else None
    t_aa = float(aex.iloc[i_aa]["time_s"]) if i_aa is not None else None

    if t_dd is None and t_aa is None:
        return None
    if t_dd is None:
        return t_aa
    if t_aa is None:
        return t_dd
    return min(t_dd, t_aa)


def trim_to_prebleach(traj: pd.DataFrame,
                      ia_col: str = "_Ia",
                      baseline_n: int = 10,
                      min_frac: float = 0.2,
                      consec: int = 3) -> pd.DataFrame:
    x = traj[ia_col].astype(float).to_numpy()
    if x.size < max(baseline_n, consec + 1):
        return traj

    base = np.nanmedian(x[:baseline_n])
    if not np.isfinite(base) or base <= 0:
        return traj

    thr = min_frac * base
    low = x < thr

    run = 0
    for i in range(low.size):
        run = run + 1 if (low[i] and np.isfinite(x[i])) else 0
        if run >= consec:
            bleach_idx = i - consec + 1
            if bleach_idx <= 0:
                return traj.iloc[:0]
            return traj.iloc[:bleach_idx]

    return traj


# =========================
# PARALLEL WORKERS (NEW)
# =========================

def _inspect_one_file(args_tuple):
    (path, frame_interval, fret_min, fret_max, key, save_plots, plot_dir, show_plots, manual_mode,
     alpha_, delta_, gamma_, S_min_, S_max_) = args_tuple

    import matplotlib
    matplotlib.use("Agg")

    # Everything below is your original per-file loop body (unchanged),
    # lifted into a worker.

    logger.info("=" * 80)

    fname = path.stem
    parts = fname.split("-")
    if len(parts) >= 3:
        exp_id = parts[1]
        construct = parts[2].split(".")[0]
    else:
        exp_id = "unknown"
        construct = "unknown"

    logger.info(f"File: {path.name}")
    logger.info(f"Experiment: {construct}, Date/ID: {exp_id}")

    try:
        store = pd.HDFStore(path, mode="r")
        keys = store.keys()
        store.close()
        logger.info(f"Keys in file: {keys}")
    except Exception as e:
        logger.error(f"Could not open file: {e}")
        return None

    k = key if key in keys else keys[0]
    try:
        df = pd.read_hdf(path, key=k)
    except Exception as e:
        logger.error(f"Error reading {k}: {e}")
        return None

    if isinstance(df.columns, pd.MultiIndex):
        df.columns = ["_".join(filter(None, col)).strip() for col in df.columns]

    logger.info(f"Loaded with shape: {df.shape}")

    num_cols = df.select_dtypes(include=[np.number]).columns
    summary = df[num_cols].describe().T[["mean", "std", "min", "max"]]
    logger.debug(summary.head(10))

    if "donor_frame" in df.columns:
        df["time_s"] = df["donor_frame"] * frame_interval

    if {"fret_f_dd", "fret_f_da", "fret_exc_type"}.issubset(df.columns):
        dex = df["fret_exc_type"] == "d"
        df.loc[dex, "E_raw"] = df.loc[dex, "fret_f_da"] / (df.loc[dex, "fret_f_da"] + df.loc[dex, "fret_f_dd"])
        fret_col = "E_raw"
        _ = fret_col  # keep same behavior

    if "time_s" not in df.columns:
        if "donor_frame" in df.columns:
            df["time_s"] = df["donor_frame"] * frame_interval
        else:
            logger.warning("No donor_frame/time info found. Skipping plot.\n")
            return None

    part_col = "fret_particle" if "fret_particle" in df.columns else None

    if part_col is not None:
        counts = df.groupby(part_col).size()
        longest_pid = counts.sort_values(ascending=False).index[0]
        traj = df[df[part_col] == longest_pid].sort_values("time_s")
        label = f"{construct} {exp_id} – particle {int(longest_pid)}"
    else:
        traj = df.sort_values("time_s")
        label = f"{construct} {exp_id} – all data (no particle id)"

    has_alex = {"fret_f_dd", "fret_f_da", "fret_f_aa", "fret_exc_type"}.issubset(df.columns)

    if not has_alex:
        logger.warning("No ALEX columns found; skipping plot.\n")
        return None

    df = df[df["fret_exc_type"].isin(["d", "a"])].copy()

    if "filter_manual" in df.columns:
        if manual_mode == "accepted":
            accepted_pids = df.loc[df["filter_manual"] == 1, "fret_particle"].dropna().unique()
            df = df[df["fret_particle"].isin(accepted_pids)].copy()
            df = df[df["filter_manual"] != 0].copy()
        elif manual_mode == "nonrejected":
            df = df[df["filter_manual"] != 0].copy()
        elif manual_mode == "all":
            pass

    if "time_s" not in df.columns:
        if "donor_frame" in df.columns:
            df["time_s"] = df["donor_frame"] * frame_interval
        else:
            logger.warning("No donor_frame/time info found. Skipping plot.\n")
            return None

    part_col = "fret_particle" if "fret_particle" in df.columns else None
    if part_col is not None:
        counts = df.groupby(part_col).size()
        longest_pid = counts.sort_values(ascending=False).index[0]
        traj = df[df[part_col] == longest_pid].sort_values("time_s").copy()
        label = f"{construct} {exp_id} – particle {int(longest_pid)}"
    else:
        traj = df.sort_values("time_s").copy()
        label = f"{construct} {exp_id} – all data (no particle id)"

    # Use passed correction factors
    try:
        traj2 = compute_corrected_E_S(
            traj, alpha=alpha_, delta=delta_, gamma=gamma_, beta=beta,
            bg_DD=bg_DD, bg_DA=bg_DA, bg_AA=bg_AA
        )
    except Exception as e:
        logger.warning(f"Could not compute corrected E/S for plotting: {e}")
        traj2 = traj.copy()
        traj2["E"] = np.nan
        traj2["S"] = np.nan

    dex = traj2["fret_exc_type"] == "d"
    aex = traj2["fret_exc_type"] == "a"

    traj2["E_raw"] = np.nan
    denom_raw = traj2.loc[dex, "fret_f_da"] + traj2.loc[dex, "fret_f_dd"]
    traj2.loc[dex, "E_raw"] = traj2.loc[dex, "fret_f_da"] / denom_raw.replace(0, np.nan)

    fig, ax = plt.subplots()
    ax.plot(traj2.loc[dex, "time_s"], traj2.loc[dex, "E_raw"], marker="o", linestyle="-", markersize=2, label="E_raw (Dex)")
    ax.plot(traj2.loc[dex, "time_s"], traj2.loc[dex, "E"],     marker="o", linestyle="-", markersize=2, label="E_corr (Dex)")
    ax.set_xlabel("time (s)")
    ax.set_ylabel("FRET E")
    ax.set_title(label + " – Dex E(t)")
    ax.set_ylim(fret_min - 0.05, fret_max + 0.05)
    ax.legend()
    fig.tight_layout()

    if save_plots and plot_dir is not None:
        safe_label = f"{construct}_{exp_id}"
        if part_col is not None:
            safe_label += f"_p{int(longest_pid)}"
        fig.savefig(Path(plot_dir) / f"{safe_label}_E.png", dpi=300)
    if show_plots:
        plt.show()
    else:
        plt.close(fig)

    fig, ax = plt.subplots()
    ax.plot(traj2["time_s"], traj2["S"], marker="o", linestyle="-", markersize=2)
    ax.axhline(S_min_, linestyle="--")
    ax.axhline(S_max_, linestyle="--")
    ax.set_xlabel("time (s)")
    ax.set_ylabel("Stoichiometry S")
    ax.set_title(label + " – S(t)")
    ax.set_ylim(-0.05, 1.05)
    fig.tight_layout()

    if save_plots and plot_dir is not None:
        safe_label = f"{construct}_{exp_id}"
        if part_col is not None:
            safe_label += f"_p{int(longest_pid)}"
        fig.savefig(Path(plot_dir) / f"{safe_label}_S.png", dpi=300)
    if show_plots:
        plt.show()
    else:
        plt.close(fig)

    fig, ax = plt.subplots()
    ax.plot(traj2.loc[dex, "time_s"], traj2.loc[dex, "fret_f_dd"], marker="o", linestyle="-", markersize=2, label="DD (Dex)")
    ax.plot(traj2.loc[dex, "time_s"], traj2.loc[dex, "fret_f_da"], marker="o", linestyle="-", markersize=2, label="DA (Dex)")
    ax.plot(traj2.loc[aex, "time_s"], traj2.loc[aex, "fret_f_aa"], marker="o", linestyle="-", markersize=2, label="AA (Aex)")
    ax.set_xlabel("time (s)")
    ax.set_ylabel("Intensity (a.u.)")
    ax.set_title(label + " – intensities by excitation")
    ax.legend()
    fig.tight_layout()

    if save_plots and plot_dir is not None:
        safe_label = f"{construct}_{exp_id}"
        if part_col is not None:
            safe_label += f"_p{int(longest_pid)}"
        fig.savefig(Path(plot_dir) / f"{safe_label}_intensities.png", dpi=300)
    if show_plots:
        plt.show()
    else:
        plt.close(fig)

    return path.name


def _export_one_tracks_file(args_tuple):
    (path, export_dir, frame_interval, fret_min, fret_max, key,
     min_traj_length, trim_prebleach, require_S_fraction, manual_mode,
     alpha_, delta_, gamma_, S_min_, S_max_,
     BLEACH_MIN_FRAC_, BLEACH_CONSEC_, BASELINE_N_) = args_tuple

    import matplotlib
    matplotlib.use("Agg")

    export_dir = Path(export_dir)
    export_dir.mkdir(parents=True, exist_ok=True)

    required_cols = {
        "donor_frame", "fret_particle", "fret_exc_type",
        "fret_f_dd", "fret_f_da", "fret_f_aa"
    }

    fname = path.stem
    parts = fname.split("-")
    if len(parts) >= 3:
        exp_id = parts[1]
        construct = parts[2].split(".")[0]
    else:
        exp_id = "unknown"
        construct = "unknown"

    try:
        raw = pd.read_hdf(path, key=key)
        if isinstance(raw.columns, pd.MultiIndex):
            raw.columns = ["_".join([x for x in c if x]).strip() for c in raw.columns]

        logger.info(
            f"{path.name}: filter_manual value_counts:\n{raw['filter_manual'].value_counts(dropna=False) if 'filter_manual' in raw.columns else 'NO COLUMN'}")
        logger.info(
            f"{path.name}: fret_exc_type value_counts:\n{raw['fret_exc_type'].value_counts(dropna=False) if 'fret_exc_type' in raw.columns else 'NO COLUMN'}")

        df = pd.read_hdf(path, key=key)
    except Exception as e:
        logger.error(f"Could not read {path.name}: {e}")
        return None

    if isinstance(df.columns, pd.MultiIndex):
        df.columns = ["_".join([x for x in c if x]).strip() for c in df.columns]

    if not required_cols.issubset(df.columns):
        missing = sorted(required_cols - set(df.columns))
        logger.warning(f"Missing required columns in {path.name}: {missing}. Skipping.")
        return None

    df["time_s"] = df["donor_frame"].astype(float) * float(frame_interval)
    # sanity: donor_frame spacing should look like ~1 cycle
    df_dex = df[df["fret_exc_type"] == "d"].sort_values("donor_frame")
    if len(df_dex) > 5:
        step = np.nanmedian(np.diff(df_dex["donor_frame"].to_numpy()))
        logger.info(f"{path.name}: median donor_frame step (Dex) = {step}")

    df = df[df["fret_exc_type"].isin(["d", "a"])].copy()

    if "filter_manual" in df.columns:
        if manual_mode == "accepted":
            accepted_pids = df.loc[df["filter_manual"] == 1, "fret_particle"].dropna().unique()
            df = df[df["fret_particle"].isin(accepted_pids)].copy()
            df = df[df["filter_manual"] != 0].copy()
        elif manual_mode == "nonrejected":
            df = df[df["filter_manual"] != 0].copy()
        elif manual_mode == "all":
            pass

    df = df[df["fret_exc_type"].isin(["d", "a"])]
    logger.info(f"{path.name}: after manual filter -> rows={len(df)} "
                f"dex={(df['fret_exc_type'] == 'd').sum()} aex={(df['fret_exc_type'] == 'a').sum()}")

    df = compute_corrected_E_S(
        df, alpha=alpha_, delta=delta_, gamma=gamma_, beta=beta,
        bg_DD=bg_DD, bg_DA=bg_DA, bg_AA=bg_AA
    )

    logger.info(f"{path.name}: rows after manual+exc filter = {len(df)}")
    logger.info(
        f"{path.name}: Dex rows = {(df['fret_exc_type'] == 'd').sum()}, Aex rows = {(df['fret_exc_type'] == 'a').sum()}")
    dex_mask = df["fret_exc_type"] == "d"
    logger.info(f"{path.name}: Dex rows with finite S = {np.isfinite(df.loc[dex_mask, 'S']).sum()} / {dex_mask.sum()}")

    logger.info(
        f"{path.name}: finite E (Dex) = {np.isfinite(df.loc[df['fret_exc_type'] == 'd', 'E']).sum() if len(df) else 0}")

    if len(df) and "S" in df.columns and df["S"].notna().any():
        logger.info(
            f"{path.name}: S stats: min={df['S'].min(skipna=True):.3f}, "
            f"med={df['S'].median(skipna=True):.3f}, "
            f"max={df['S'].max(skipna=True):.3f}"
        )
    else:
        logger.info(f"{path.name}: S stats: n/a (empty or all-NaN)")

    plot_file_qc(df, outdir=export_dir / "plots_qc", tag=path.stem, n_particles=30)

    df["_S_ok"] = (df["S"] >= S_min_) & (df["S"] <= S_max_)

    count = 0
    for pid, traj in df.groupby("fret_particle"):
        traj = traj.sort_values("donor_frame").copy()

        # if trim_prebleach:
        #     aex = traj[traj["fret_exc_type"] == "a"].copy()
        #     if aex.shape[0] >= max(BASELINE_N_, BLEACH_CONSEC_ + 1):
        #         aex["_AA"] = aex["fret_f_aa"].astype(float)
        #         aex_trim = trim_to_prebleach(
        #             aex,
        #             ia_col="_AA",
        #             baseline_n=BASELINE_N_,
        #             min_frac=BLEACH_MIN_FRAC_,
        #             consec=BLEACH_CONSEC_,
        #         )
        #         if aex_trim.shape[0] == 0:
        #             continue
        #         bleach_time = float(aex_trim["time_s"].max())
        #         traj = traj[traj["time_s"] <= bleach_time].copy()

        if trim_prebleach:
            bt = detect_first_bleach_time(
                traj,
                baseline_n=BASELINE_N_,
                min_frac=BLEACH_MIN_FRAC_,
                consec=BLEACH_CONSEC_,
            )
            if bt is None:
                continue
            traj = traj[traj["time_s"] <= bt].copy()

        finite_S = np.isfinite(traj["S"].to_numpy())
        if finite_S.sum() == 0:
            continue

        s_ok = (traj["S"].to_numpy() >= S_min_) & (traj["S"].to_numpy() <= S_max_) & finite_S
        frac_ok = s_ok.sum() / finite_S.sum()
        if frac_ok < require_S_fraction:
            continue

        dex = traj[traj["fret_exc_type"] == "d"].copy()

        dex = dex[np.isfinite(dex["E"])]
        dex = dex[(dex["E"] >= fret_min) & (dex["E"] <= fret_max)]

        if dex.shape[0] < min_traj_length:
            continue

        out = dex[["time_s", "E", "S"]].copy()
        out_name = f"{path.stem}_particle_{int(pid):05d}.csv"
        out.to_csv(export_dir / out_name, index=False)
        count += 1

    logger.info(f"Exported {count} per-particle traces from {path.name} to {export_dir}/")
    return (path.name, count)


def _interp_one_csv(args_tuple):
    i, f, time_grid, use_interpolation = args_tuple
    df = pd.read_csv(f)
    if len(df) == 0:
        return (i, None, None)

    t_trace = df["time_s"].values
    E_trace = df["E"].values

    interp = interpolate_trace(time_grid, t_trace, E_trace, interpolate=use_interpolation)

    stem = f.stem
    parts = stem.split("-")
    if len(parts) >= 3:
        exp_id = parts[1]
        construct = parts[2].split(".")[0]
    else:
        exp_id = "unknown"
        construct = "unknown"

    pid = stem.split("particle_")[-1]
    col_name = f"{construct}_{exp_id}_p{pid}"

    return (i, col_name, interp)


# =========================
# ORIGINAL FUNCTIONS, NOW WITH PARALLEL OUTER LOOPS
# =========================

def inspect_and_plot_data(
        data_dir: Path,
        frame_interval: float,
        fret_min: float,
        fret_max: float,
        key: str = "/tracks/Data",
        save_plots: bool = True,
        plot_dir: Path | None = None,
        show_plots: bool = False,
        manual_mode: str = "accepted",
) -> None:
    if save_plots:
        if plot_dir is None:
            plot_dir = data_dir / "plots"
        plot_dir.mkdir(parents=True, exist_ok=True)

    paths = sorted(data_dir.glob("*.tracks*"))
    if not paths:
        return

    jobs = getattr(inspect_and_plot_data, "_n_jobs", 1)

    if jobs <= 1:
        for path in paths:
            _inspect_one_file(
                (path, frame_interval, fret_min, fret_max, key, save_plots, plot_dir, show_plots, manual_mode,
                 alpha, delta, gamma, S_min, S_max)
            )
        return

    work = [
        (p, frame_interval, fret_min, fret_max, key, save_plots, plot_dir, show_plots, manual_mode,
         alpha, delta, gamma, S_min, S_max)
        for p in paths
    ]

    with ProcessPoolExecutor(max_workers=jobs) as ex:
        futures = [ex.submit(_inspect_one_file, w) for w in work]
        for fut in as_completed(futures):
            try:
                fut.result()
            except Exception as e:
                logger.error(f"[inspect] worker failed: {e}")


def export_per_particle_time_series(
    data_dir: Path,
    export_dir: Path,
    frame_interval: float,
    fret_min: float,
    fret_max: float,
    key: str = "/tracks/Data",
    min_traj_length: int = 20,
    trim_prebleach: bool = False,
    require_S_fraction: float = 0.75,
    manual_mode: str = "accepted"
) -> None:
    export_dir.mkdir(parents=True, exist_ok=True)

    paths = sorted(data_dir.glob("*.tracks*.h5"))
    if not paths:
        return

    jobs = getattr(export_per_particle_time_series, "_n_jobs", 1)

    if jobs <= 1:
        for path in paths:
            _export_one_tracks_file(
                (path, export_dir, frame_interval, fret_min, fret_max, key,
                 min_traj_length, trim_prebleach, require_S_fraction, manual_mode,
                 alpha, delta, gamma, S_min, S_max,
                 BLEACH_MIN_FRAC, BLEACH_CONSEC, BASELINE_N)
            )
        return

    work = [
        (p, export_dir, frame_interval, fret_min, fret_max, key,
         min_traj_length, trim_prebleach, require_S_fraction, manual_mode,
         alpha, delta, gamma, S_min, S_max,
         BLEACH_MIN_FRAC, BLEACH_CONSEC, BASELINE_N)
        for p in paths
    ]

    with ProcessPoolExecutor(max_workers=jobs) as ex:
        futures = [ex.submit(_export_one_tracks_file, w) for w in work]
        for fut in as_completed(futures):
            try:
                fut.result()
            except Exception as e:
                logger.error(f"[export] worker failed: {e}")


def build_combined_fret_matrix(
        export_dir: Path,
        frame_interval: float,
        fret_min: float,
        fret_max: float,
        use_interpolation: bool,
        combined_out: Path | None = None,
) -> Path | None:
    if combined_out is None:
        combined_out = export_dir / "fret_matrix.csv"

    logger.info("\nBuilding combined FRET matrix (uniform 0–max_t grid)...")

    csv_files = sorted(
        f for f in export_dir.glob("*.csv") if f.name != combined_out.name
    )

    if not csv_files:
        logger.warning("No per-particle CSV files found, skipping matrix creation.")
        return None

    max_t = 0.0
    for f in csv_files:
        df_tmp = pd.read_csv(f)
        if "time_s" in df_tmp.columns and len(df_tmp) > 0:
            max_t = max(max_t, df_tmp["time_s"].max())

    time_grid = np.arange(0.0, max_t + frame_interval / 2, frame_interval)

    columns: dict[str, np.ndarray] = {"time_s": time_grid}

    jobs = getattr(build_combined_fret_matrix, "_n_jobs", 1)

    if jobs <= 1:
        for i, f in enumerate(csv_files):
            i2, col_name, interp = _interp_one_csv((i, f, time_grid, use_interpolation))
            if col_name is not None:
                columns[col_name] = interp
    else:
        work = [(i, f, time_grid, use_interpolation) for i, f in enumerate(csv_files)]
        results = []
        with ProcessPoolExecutor(max_workers=jobs) as ex:
            futures = [ex.submit(_interp_one_csv, w) for w in work]
            for fut in as_completed(futures):
                results.append(fut.result())

        for i2, col_name, interp in sorted(results, key=lambda x: x[0]):
            if col_name is not None:
                columns[col_name] = interp

    combined = pd.DataFrame(columns)

    if "time_s" not in combined.columns:
        raise ValueError("Expected 'time_s' column in combined matrix.")

    traj_cols = [c for c in combined.columns if c != "time_s"]

    traj_cols = [c for c in traj_cols if combined[c].notna().any()]
    combined = combined[["time_s"] + traj_cols]

    for c in traj_cols:
        mask_invalid = (combined[c] < fret_min) | (combined[c] > fret_max)
        combined.loc[mask_invalid, c] = np.nan

    if traj_cols:
        mask_any = combined[traj_cols].notna().any(axis=1)
        combined = combined[mask_any].reset_index(drop=True)

    min_points_per_trace = 10
    if traj_cols:
        valid_counts = combined[traj_cols].notna().sum(axis=0)
        keep_traces = valid_counts[valid_counts >= min_points_per_trace].index.tolist()
        combined = combined[["time_s"] + keep_traces]
        traj_cols = keep_traces
    else:
        traj_cols = []

    combined.to_csv(combined_out, index=False)
    logger.info(
        f"FRET matrix saved → {combined_out}\n"
        f"Time points: {combined.shape[0]}, trajectories: {combined.shape[1] - 1}"
    )

    return combined_out


def cleanup_intermediate_csv(export_dir: Path, combined_name: str = "fret_matrix.csv") -> None:
    for f in export_dir.glob("*.csv"):
        if f.name != combined_name:
            try:
                f.unlink()
            except Exception as e:
                logger.error(f"Could not delete {f.name}: {e}")


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description=(
            "HDF5 FRET Tracking Data Processor\n"
            "Inspect raw tracks, export per-particle traces and build a combined matrix."
        )
    )

    parser.add_argument("--data-dir", type=Path, default=Path("data/Hugel_2025"))
    parser.add_argument("--export-dir", type=Path, default=Path("data/timeseries"))
    parser.add_argument("--frame-interval", type=float, default=0.18)
    parser.add_argument("--fret-min", type=float, default=0.0)
    parser.add_argument("--fret-max", type=float, default=1.0)
    parser.add_argument("--min-traj-length", type=int, default=10)
    parser.add_argument("--use-interpolation", action="store_true")
    parser.add_argument("--no-inspect-plots", action="store_true")
    parser.add_argument("--save-plots", action="store_true")
    parser.add_argument("--plots-dir", type=Path, default="data/plots/")
    parser.add_argument("--keep-intermediate", action="store_true")

    # NEW
    parser.add_argument(
        "--n-jobs",
        type=int,
        default=os.cpu_count(),
        help="Number of parallel worker processes (default: number of CPUs available).",
    )

    parser.add_argument("--alpha", type=float, default=0.17)
    parser.add_argument("--delta", type=float, default=0.12)
    parser.add_argument("--gamma", type=float, default=0.8)

    parser.add_argument("--bg-DD", type=float, default=0.0)
    parser.add_argument("--bg-DA", type=float, default=0.0)
    parser.add_argument("--bg-AA", type=float, default=0.0)

    parser.add_argument("--S-min", type=float, default=0.35)
    parser.add_argument("--S-max", type=float, default=0.60)

    parser.add_argument("--trim-prebleach", action="store_true")
    parser.add_argument("--bleach-min-frac", type=float, default=0.20)
    parser.add_argument("--bleach-consec", type=int, default=3)
    parser.add_argument("--baseline-n", type=int, default=10)

    parser.add_argument("--require-S-fraction", type=float, default=0.75)

    parser.add_argument("--manual-mode", choices=["accepted", "nonrejected", "all"], default="accepted")

    return parser.parse_args()


def main() -> None:
    global data_dir, export_dir, frame_interval, fret_min, fret_max, USE_INTERPOLATION
    global alpha, delta, gamma, S_min, S_max, BLEACH_MIN_FRAC, BLEACH_CONSEC, BASELINE_N
    global alpha, delta, gamma, beta, bg_DD, bg_DA, bg_AA

    args = parse_args()

    alpha = args.alpha
    delta = args.delta
    gamma = args.gamma

    bg_DD = args.bg_DD
    bg_DA = args.bg_DA
    bg_AA = args.bg_AA

    S_min = args.S_min
    S_max = args.S_max

    BLEACH_MIN_FRAC = args.bleach_min_frac
    BLEACH_CONSEC = args.bleach_consec
    BASELINE_N = args.baseline_n

    data_dir = args.data_dir
    export_dir = args.export_dir
    frame_interval = args.frame_interval
    fret_min = args.fret_min
    fret_max = args.fret_max
    USE_INTERPOLATION = args.use_interpolation

    # Wire worker counts without changing function signatures
    inspect_and_plot_data._n_jobs = args.n_jobs
    export_per_particle_time_series._n_jobs = args.n_jobs
    build_combined_fret_matrix._n_jobs = args.n_jobs

    logger.info("=== HDF5 FRET Tracking Data Processor ===")
    logger.info(f"Data directory     : {data_dir}")
    logger.info(f"Export directory   : {export_dir}")
    logger.info(f"Frame interval     : {frame_interval} s")
    logger.info(f"FRET range         : [{fret_min}, {fret_max}]")
    logger.info(f"Use interpolation  : {USE_INTERPOLATION}")
    logger.info(f"Min traj length    : {args.min_traj_length}")
    logger.info(f"Save plots         : {args.save_plots}")
    logger.info(f"Keep intermediates : {args.keep_intermediate}")
    logger.info(f"Parallel workers   : {args.n_jobs}")

    if not args.no_inspect_plots:
        inspect_and_plot_data(
            data_dir=data_dir,
            frame_interval=frame_interval,
            fret_min=fret_min,
            fret_max=fret_max,
            key=key,
            save_plots=args.save_plots,
            plot_dir=args.plots_dir,
            show_plots=False,
            manual_mode=args.manual_mode,
        )

    export_per_particle_time_series(
        data_dir=data_dir,
        export_dir=export_dir,
        frame_interval=frame_interval,
        fret_min=fret_min,
        fret_max=fret_max,
        key=key,
        min_traj_length=args.min_traj_length,
        trim_prebleach=args.trim_prebleach,
        require_S_fraction=args.require_S_fraction,
        manual_mode=args.manual_mode,
    )

    combined_path = build_combined_fret_matrix(
        export_dir=export_dir,
        frame_interval=frame_interval,
        fret_min=fret_min,
        fret_max=fret_max,
        use_interpolation=USE_INTERPOLATION,
        combined_out=export_dir / "fret_matrix.csv",
    )

    plot_construct_summary(export_dir, export_dir / "plots_construct")

    if combined_path is not None and not args.keep_intermediate:
        cleanup_intermediate_csv(export_dir, combined_name=combined_path.name)


if __name__ == "__main__":
    main()