# SPDX-License-Identifier: BSD-3-Clause
# SPDX-FileCopyrightText: 2025 Abhinav Mishra

"""
Figures and visual outputs generated by this script are licensed under:
SPDX-License-Identifier: CC-BY-4.0
"""

# BSD 3-Clause License
#
# Copyright (c) 2025, Abhinav Mishra
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:
#
# 1. Redistributions of source code must retain the above copyright notice, this
#    list of conditions and the following disclaimer.
#
# 2. Redistributions in binary form must reproduce the above copyright notice,
#    this list of conditions and the following disclaimer in the documentation
#    and/or other materials provided with the distribution.
#
# 3. Neither the name of the copyright holder nor the names of its
#    contributors may be used to endorse or promote products derived from
#    this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

# HDF5 FRET Tracking Data Processor
# =================================
#
# This script processes single-molecule FRET tracking data stored in HDF5 format.
# It is designed to inspect raw data, filter and export individual particle
# trajectories, and combine them into a unified time-series matrix.


import argparse
import logging
import warnings
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from scipy.interpolate import CubicSpline

# === Logger Setup ===
logging.basicConfig(level=logging.INFO, format="%(message)s")
logger = logging.getLogger(__name__)

warnings.filterwarnings("ignore", category=UserWarning, module="pandas")

# === Global configuration (overridden by CLI in main) ===
data_dir = Path("data/Hugel_2025")   # adjusted via --data-dir
export_dir = Path("data/timeseries") # adjusted via --export-dir
key = "/tracks/Data"                 # default key
frame_interval = 0.07                # seconds per frame (70 ms, as in Anandamurugan et al. 2026)
fret_max = 1.0                       # max FRET efficiency to consider
fret_min = 0.0                       # min FRET efficiency to consider
USE_INTERPOLATION = False            # Set to False - disable cubic splines/filling


def interpolate_trace(
    time_grid: np.ndarray,
    t_trace: np.ndarray,
    E_trace: np.ndarray,
    interpolate: bool = True,
) -> np.ndarray:
    """
    Interpolate a single FRET trace onto a common time grid using cubic splines.

    Handles edge cases such as missing data (NaNs), single-point traces,
    and short traces where only linear interpolation is feasible.

    Args:
        time_grid (np.ndarray): The uniform time vector to interpolate onto.
        t_trace (np.ndarray): Original time points of the trace.
        E_trace (np.ndarray): Original FRET efficiency values.

    Returns:
        np.ndarray: The FRET trace interpolated onto `time_grid`. Returns NaNs
        for time points outside the original observation range or if
        insufficient data exists.
    """
    # Ensure arrays
    t_trace = np.asarray(t_trace, float)
    E_trace = np.asarray(E_trace, float)

    # Mask out non-finite values
    mask = np.isfinite(t_trace) & np.isfinite(E_trace)
    t_clean = t_trace[mask]
    E_clean = E_trace[mask]

    # Not enough points to interpolate
    if t_clean.size == 0:
        return np.full_like(time_grid, np.nan, dtype=float)
    if t_clean.size == 1:
        # Single point: constant over its range, NaN elsewhere
        y = np.full_like(time_grid, np.nan, dtype=float)
        idx = np.argmin(np.abs(time_grid - t_clean[0]))
        y[idx] = E_clean[0]
        return y

    # Remove duplicate time stamps, if any
    t_unique, idx_unique = np.unique(t_clean, return_index=True)
    E_unique = E_clean[idx_unique]

    if not interpolate:
        y = np.full_like(time_grid, np.nan, dtype=float)
        if len(time_grid) > 1:
            dt = time_grid[1] - time_grid[0]
            # Find nearest grid index for each observation
            idx = np.rint(t_unique / dt).astype(np.int64)
            # Ensure indices are within bounds
            valid = (idx >= 0) & (idx < len(time_grid))
            y[idx[valid]] = E_unique[valid]
        elif len(time_grid) == 1 and t_unique.size > 0:
            # Edge case: 1-point grid, just take the first point if close enough
            if np.abs(t_unique[0] - time_grid[0]) < frame_interval / 2:
                y[0] = E_unique[0]
        return y

    if t_unique.size < 2:
        return np.full_like(time_grid, np.nan, dtype=float)

    # For 2 points, a spline is pointless; use linear interpolation
    if t_unique.size == 2:
        y = np.interp(time_grid, t_unique, E_unique, left=np.nan, right=np.nan)
        return y

    # For >=3 points, try cubic spline
    try:
        cs = CubicSpline(t_unique, E_unique, extrapolate=False)
        y = cs(time_grid)
    except Exception:
        y = np.interp(time_grid, t_unique, E_unique, left=np.nan, right=np.nan)

    outside = (time_grid < t_unique.min()) | (time_grid > t_unique.max())
    y[outside] = np.nan
    y[(y < fret_min) | (y > fret_max)] = np.nan
    return y


def inspect_and_plot_data(
    data_dir: Path,
    frame_interval: float,
    fret_min: float,
    fret_max: float,
    key: str = "/tracks/Data",
    save_plots: bool = True,
    plot_dir: Path | None = None,
    show_plots: bool = False,
) -> None:
    """
    Inspect all *.tracks* files, log basic info and
    plot a representative trajectory per file.
    Optionally save plots to disk.
    """
    if save_plots:
        if plot_dir is None:
            plot_dir = data_dir / "plots"
        plot_dir.mkdir(parents=True, exist_ok=True)

    for path in sorted(data_dir.glob("*.tracks*")):
        logger.info("=" * 80)

        # Extract metadata from filename
        fname = path.stem  # e.g. filtered-241107-Hsp90_409_601-v014.tracks
        parts = fname.split("-")
        if len(parts) >= 3:
            exp_id = parts[1]
            construct = parts[2].split(".")[0]
        else:
            exp_id = "unknown"
            construct = "unknown"

        logger.info(f"File: {path.name}")
        logger.info(f"Experiment: {construct}, Date/ID: {exp_id}")

        # Step 1 — list keys
        try:
            store = pd.HDFStore(path, mode="r")
            keys = store.keys()
            store.close()
            logger.info(f"Keys in file: {keys}")
        except Exception as e:
            logger.error(f"Could not open file: {e}")
            continue

        # Step 2 — read dataset
        k = key if key in keys else keys[0]
        try:
            df = pd.read_hdf(path, key=k)
        except Exception as e:
            logger.error(f"Error reading {k}: {e}")
            continue

        # Step 3 — flatten multiindex columns
        if isinstance(df.columns, pd.MultiIndex):
            df.columns = ["_".join(filter(None, col)).strip() for col in df.columns]

        # Step 4 — basic info
        logger.info(f"Loaded with shape: {df.shape}")

        # Step 6 — numeric summary
        num_cols = df.select_dtypes(include=[np.number]).columns
        summary = df[num_cols].describe().T[["mean", "std", "min", "max"]]
        logger.debug(summary.head(10))

        # Step 7 — add time column
        if "donor_frame" in df.columns:
            df["time_s"] = df["donor_frame"] * frame_interval

        # Step 8 — detect FRET column
        fret_candidates = ["fret_eff", "fret_eff_app", "fret_efficiency"]
        fret_col = next((c for c in fret_candidates if c in df.columns), None)

        if fret_col is None:
            logger.warning(
                f"No FRET column found (checked: {', '.join(fret_candidates)}). Skipping plot.\n"
            )
            continue

        # Ensure we have a time axis
        if "time_s" not in df.columns:
            if "donor_frame" in df.columns:
                df["time_s"] = df["donor_frame"] * frame_interval
            else:
                logger.warning("No donor_frame/time info found. Skipping plot.\n")
                continue

        # Choose a representative particle: longest trajectory
        part_col = "fret_particle" if "fret_particle" in df.columns else None

        if part_col is not None:
            counts = df.groupby(part_col).size()
            longest_pid = counts.sort_values(ascending=False).index[0]
            traj = df[df[part_col] == longest_pid].sort_values("time_s")
            label = f"{construct} {exp_id} – particle {int(longest_pid)}"
        else:
            traj = df.sort_values("time_s")
            label = f"{construct} {exp_id} – all data (no particle id)"

        # Plot representative trace
        fig, ax = plt.subplots()
        ax.plot(traj["time_s"], traj[fret_col], marker="o", linestyle="-", markersize=3)
        ax.set_xlabel("time (s)")
        ax.set_ylabel(f"{fret_col}")
        ax.set_title(label)
        fig.tight_layout()

        if save_plots and plot_dir is not None:
            # create a file-name–safe stem
            safe_label = f"{construct}_{exp_id}"
            if part_col is not None:
                safe_label += f"_p{int(longest_pid)}"
            out_png = plot_dir / f"{safe_label}.png"
            fig.savefig(out_png, dpi=300)
            logger.info(f"Saved plot → {out_png}")

        if show_plots:
            plt.show()
        else:
            plt.close(fig)


def export_per_particle_time_series(
    data_dir: Path,
    export_dir: Path,
    frame_interval: float,
    fret_min: float,
    fret_max: float,
    key: str = "/tracks/Data",
    min_traj_length: int = 20,
) -> None:
    """
    Export per-particle time series as CSV files for downstream combination.
    """
    export_dir.mkdir(parents=True, exist_ok=True)

    for path in sorted(data_dir.glob("*.tracks*.h5")):
        fname = path.stem
        parts = fname.split("-")
        if len(parts) >= 3:
            exp_id = parts[1]
            construct = parts[2].split(".")[0]
        else:
            exp_id = "unknown"
            construct = "unknown"

        try:
            df = pd.read_hdf(path, key=key)
        except Exception as e:
            logger.error(f"Could not read {path.name}: {e}")
            continue

        # flatten columns
        if isinstance(df.columns, pd.MultiIndex):
            df.columns = ["_".join(filter(None, c)).strip() for c in df.columns]

        # --- NEW: restrict to donor excitation frames if available ---
        if "fret_exc_type" in df.columns:
            df = df[df["fret_exc_type"] == "d"]

        # ensure required columns
        needed_cols = {"donor_frame", "fret_particle"}
        if not needed_cols.issubset(df.columns):
            logger.warning(f"Missing required columns in {path.name}, skipping.")
            continue

        df["time_s"] = df["donor_frame"] * frame_interval

        fret_candidates = ["fret_eff", "fret_eff_app"]
        fret_col = next((c for c in fret_candidates if c in df.columns), None)
        if fret_col is None:
            logger.warning(f"No FRET efficiency column found in {path.name}, skipping file.")
            continue

        # --- FIXED: manual filter semantics ---
        # manual == 1 means "rejected" in the original GUI
        if "filter_manual" in df.columns:
            df = df[df["filter_manual"] == 1]

        if "fret_exec_type" in df.columns:
            df = df[df["fret_exec_type"] == 1]

        # keep only finite, physically reasonable FRET values
        df = df[np.isfinite(df[fret_col])]
        df = df[(df[fret_col] > fret_min) & (df[fret_col] < fret_max)]

        # drop short trajectories AFTER filtering
        lengths = df.groupby("fret_particle")["donor_frame"].nunique()
        keep = lengths[lengths >= min_traj_length].index
        df = df[df["fret_particle"].isin(keep)]

        count = 0
        for pid, traj in df.groupby("fret_particle"):
            traj = traj.sort_values("donor_frame")
            out = traj[["time_s", fret_col]].rename(columns={fret_col: "FRET"})
            out_name = f"{path.stem}_particle_{int(pid):05d}.csv"
            out.to_csv(export_dir / out_name, index=False)
            count += 1

        logger.info(f"Exported {count} per-particle traces from {path.name} to {export_dir}/")


def build_combined_fret_matrix(
    export_dir: Path,
    frame_interval: float,
    fret_min: float,
    fret_max: float,
    use_interpolation: bool,
    combined_out: Path | None = None,
) -> Path | None:
    """
    Combine all per-particle CSV files into a single time x trajectory matrix,
    clean it, and save to CSV.
    """
    if combined_out is None:
        combined_out = export_dir / "fret_matrix.csv"

    logger.info("\nBuilding combined FRET matrix (uniform 0–max_t grid)...")

    # Exclude the already combined matrix itself (important on reruns)
    csv_files = sorted(
        f for f in export_dir.glob("*.csv") if f.name != combined_out.name
    )

    if not csv_files:
        logger.warning("No per-particle CSV files found, skipping matrix creation.")
        return None

    max_t = 0.0

    # Find max time across all traces
    for f in csv_files:
        df_tmp = pd.read_csv(f)
        if "time_s" in df_tmp.columns and len(df_tmp) > 0:
            max_t = max(max_t, df_tmp["time_s"].max())

    time_grid = np.arange(0.0, max_t + frame_interval / 2, frame_interval)

    # Collect all interpolated traces first
    columns: dict[str, np.ndarray] = {"time_s": time_grid}

    for i, f in enumerate(csv_files):
        df = pd.read_csv(f)
        if len(df) == 0:
            continue

        t_trace = df["time_s"].values
        E_trace = df["FRET"].values
        interp = interpolate_trace(time_grid, t_trace, E_trace, interpolate=use_interpolation)

        stem = f.stem
        parts = stem.split("-")
        if len(parts) >= 3:
            exp_id = parts[1]
            construct = parts[2].split(".")[0]
        else:
            exp_id = "unknown"
            construct = "unknown"

        pid = stem.split("particle_")[-1]
        col_name = f"{construct}_{exp_id}_p{pid}"

        columns[col_name] = interp

        # if (i + 1) % 100 == 0:
        #     logger.info(f"Processed {i + 1} traces ...")

    # Build one DataFrame - time x trajectories
    combined = pd.DataFrame(columns)

    # --- CLEANUP MATRIX ---

    # 1) Make sure time_s exists
    if "time_s" not in combined.columns:
        raise ValueError("Expected 'time_s' column in combined matrix.")

    # All trajectory columns (everything except time_s)
    traj_cols = [c for c in combined.columns if c != "time_s"]

    # 2) Drop trajectory columns that are entirely NaN
    traj_cols = [c for c in traj_cols if combined[c].notna().any()]
    combined = combined[["time_s"] + traj_cols]

    # 3) Enforce valid FRET range and set out-of-range values to NaN
    for c in traj_cols:
        mask_invalid = (combined[c] < fret_min) | (combined[c] > fret_max)
        combined.loc[mask_invalid, c] = np.nan

    # 4) Drop rows where all trajectories are NaN
    if traj_cols:
        mask_any = combined[traj_cols].notna().any(axis=1)
        combined = combined[mask_any].reset_index(drop=True)

    # 5) Drop very short traces (e.g. < 10 valid points after filtering)
    min_points_per_trace = 10
    if traj_cols:
        valid_counts = combined[traj_cols].notna().sum(axis=0)
        keep_traces = valid_counts[valid_counts >= min_points_per_trace].index.tolist()
        combined = combined[["time_s"] + keep_traces]
        traj_cols = keep_traces  # update
    else:
        traj_cols = []

    # --- SAVE CLEANED MATRIX ---
    combined.to_csv(combined_out, index=False)
    logger.info(
        f"FRET matrix saved → {combined_out}\n"
        f"Time points: {combined.shape[0]}, trajectories: {combined.shape[1] - 1}"
    )

    return combined_out


def cleanup_intermediate_csv(export_dir: Path, combined_name: str = "fret_matrix.csv") -> None:
    """
    Remove all per-particle CSVs except the combined matrix.
    """
    for f in export_dir.glob("*.csv"):
        if f.name != combined_name:
            try:
                f.unlink()
            except Exception as e:
                logger.error(f"Could not delete {f.name}: {e}")


def parse_args() -> argparse.Namespace:
    """
    Parse command-line arguments for the FRET HDF5 processing script.
    """
    parser = argparse.ArgumentParser(
        description=(
            "HDF5 FRET Tracking Data Processor\n"
            "Inspect raw tracks, export per-particle traces and build a combined matrix."
        )
    )

    parser.add_argument(
        "--data-dir",
        type=Path,
        default=Path("data/Hugel_2025"),
        help="Directory containing .tracks / .tracks.h5 files (default: data/Hugel_2025)",
    )
    parser.add_argument(
        "--export-dir",
        type=Path,
        default=Path("data/timeseries"),
        help="Directory to write per-particle CSVs and combined matrix (default: data/timeseries)",
    )
    parser.add_argument(
        "--frame-interval",
        type=float,
        default=0.07,
        help="Frame interval in seconds (default: 0.07 = 70 ms)",
    )
    parser.add_argument(
        "--fret-min",
        type=float,
        default=0.0,
        help="Minimum FRET value to keep (default: 0.0)",
    )
    parser.add_argument(
        "--fret-max",
        type=float,
        default=1.0,
        help="Maximum FRET value to keep (default: 1.0)",
    )
    parser.add_argument(
        "--min-traj-length",
        type=int,
        default=10,
        help="Minimum trajectory length (in frames) to keep (default: 10)",
    )
    parser.add_argument(
        "--use-interpolation",
        action="store_true",
        help="Enable interpolation (cubic/linear) onto the common time grid.",
    )
    parser.add_argument(
        "--no-inspect-plots",
        action="store_true",
        help="Disable representative trajectory plotting.",
    )
    parser.add_argument(
        "--save-plots",
        action="store_true",
        help="Save representative trajectory plots as PNG files.",
    )
    parser.add_argument(
        "--plots-dir",
        type=Path,
        default=None,
        help="Directory to save plots (default: <data-dir>/plots).",
    )
    parser.add_argument(
        "--keep-intermediate",
        action="store_true",
        help="Do not delete per-particle CSVs after building the combined matrix.",
    )

    return parser.parse_args()


def main() -> None:
    """
    Main entry point: run inspection, export per-particle traces,
    build combined matrix, and clean up.
    """
    global data_dir, export_dir, frame_interval, fret_min, fret_max, USE_INTERPOLATION

    args = parse_args()

    # Override globals with CLI
    data_dir = args.data_dir
    export_dir = args.export_dir
    frame_interval = args.frame_interval
    fret_min = args.fret_min
    fret_max = args.fret_max
    USE_INTERPOLATION = args.use_interpolation

    logger.info("=== HDF5 FRET Tracking Data Processor ===")
    logger.info(f"Data directory     : {data_dir}")
    logger.info(f"Export directory   : {export_dir}")
    logger.info(f"Frame interval     : {frame_interval} s")
    logger.info(f"FRET range         : [{fret_min}, {fret_max}]")
    logger.info(f"Use interpolation  : {USE_INTERPOLATION}")
    logger.info(f"Min traj length    : {args.min_traj_length}")
    logger.info(f"Save plots         : {args.save_plots}")
    logger.info(f"Keep intermediates : {args.keep_intermediate}")

    # 1) Inspect and plot data (optional)
    if not args.no_inspect_plots:
        inspect_and_plot_data(
            data_dir=data_dir,
            frame_interval=frame_interval,
            fret_min=fret_min,
            fret_max=fret_max,
            key=key,
            save_plots=args.save_plots,
            plot_dir=args.plots_dir,
            show_plots=False,  # pipeline-friendly; change to True if you want interactive
        )

    # 2) Export per-particle time series
    export_per_particle_time_series(
        data_dir=data_dir,
        export_dir=export_dir,
        frame_interval=frame_interval,
        fret_min=fret_min,
        fret_max=fret_max,
        key=key,
        min_traj_length=args.min_traj_length,
    )

    # 3) Build combined FRET matrix
    combined_path = build_combined_fret_matrix(
        export_dir=export_dir,
        frame_interval=frame_interval,
        fret_min=fret_min,
        fret_max=fret_max,
        use_interpolation=USE_INTERPOLATION,
        combined_out=export_dir / "fret_matrix.csv",
    )

    # 4) Cleanup intermediate CSVs
    if combined_path is not None and not args.keep_intermediate:
        cleanup_intermediate_csv(export_dir, combined_name=combined_path.name)


if __name__ == "__main__":
    main()
